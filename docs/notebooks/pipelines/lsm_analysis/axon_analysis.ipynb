{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axon Segmentation Analysis of Whole-Brain Light-Sheet Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before Using this notebook:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Install brainlit, and other packages that this notebook uses\n",
    "### 1b. Write images to s3 using CloudReg\n",
    "    - e.g. python -m cloudreg.scripts.create_precomputed_volumes --s3_input_paths /mnt/NAS/SmartSPIM_Data/2022_03_02/20220302_14_40_04_8529_destriped_DONE/Ex_561_Em_600_stitched --s3_output_paths  s3://smartspim-precomputed-volumes/2022_03_02/8529/Ch_561_v2  --voxel_size 1.83 1.83 2 --num_procs 24 --resample_iso False\n",
    "### 1c. Make point annotations in neuroglancer to identify subvolumes for validation (and possible training)\n",
    "    - instructions: https://neurodata.io/help/neuroglancer-pt-annotations/\n",
    "### 1d. Update axon_data.py file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibody_layer = \"Ch_647\"\n",
    "background_layer = \"Ch_561\"\n",
    "endogenous_layer = \"Ch_488\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from skimage.transform import downscale_local_mean\n",
    "from skimage import io, measure\n",
    "import napari\n",
    "import random\n",
    "import h5py\n",
    "from brainlit.preprocessing import removeSmallCCs\n",
    "from brainlit.lsm_analysis.util import (\n",
    "    json_to_points,\n",
    "    find_atlas_level_label,\n",
    "    fold,\n",
    "    setup_atlas_graph,\n",
    "    get_atlas_level_nodes,\n",
    ")\n",
    "from brainlit.lsm_analysis.data.axon_data import brain2paths, brain2centers\n",
    "from brainlit.lsm_analysis.parse_ara import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import tables\n",
    "from napari_animation import AnimationWidget\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "import pandas as pd\n",
    "import brainrender\n",
    "from skimage.morphology import skeletonize\n",
    "import os\n",
    "from pathlib import Path\n",
    "import scipy.ndimage as ndi\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "%gui qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainlit_path = Path(os.path.abspath(\"\"))\n",
    "brainlit_path = brainlit_path.parents[2]\n",
    "print(f\"Path to brainlit: {brainlit_path}\")\n",
    "\n",
    "for id in brain2paths.keys():\n",
    "    if \"base\" in brain2paths[id].keys() and \"val_info\" in brain2paths[id].keys():\n",
    "        base = brain2paths[id][\"base\"]\n",
    "        if \"http\" in base:\n",
    "            print(f\"Sample {id}: http in basepath, which may cause write errors\")\n",
    "        for layer in [antibody_layer, background_layer, endogenous_layer]:\n",
    "            try:\n",
    "                CloudVolume(base + layer)\n",
    "            except:\n",
    "                print(f\"Sample {id}: Layer {layer} not found in {base}\")\n",
    "\n",
    "        try:\n",
    "            url = brain2paths[id][\"val_info\"][\"url\"]\n",
    "            layer = brain2paths[id][\"val_info\"][\"layer\"]\n",
    "            pts = json_to_points(url)[layer]\n",
    "        except:\n",
    "            print(f\"Sample {id}: Error with val_info\")\n",
    "\n",
    "        if \"train_info\" in brain2paths[id].keys():\n",
    "            try:\n",
    "                url = brain2paths[id][\"train_info\"][\"url\"]\n",
    "                layer = brain2paths[id][\"train_info\"][\"layer\"]\n",
    "                pts = json_to_points(url)[layer]\n",
    "            except:\n",
    "                print(f\"Sample {id}: Error with train_info\")\n",
    "    else:\n",
    "        print(f\"Sample {id}: Does not conform to desired format\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download benchmark data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"8589\"  # brain ID\n",
    "axon_data_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/\"  # path to directory where training/validation data should be stored\n",
    "dataset_to_save = \"val\"  # train or val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful variables\n",
    "base_dir = axon_data_dir + \"brain\" + brain + \"/\"\n",
    "\n",
    "if brain not in brain2paths.keys():\n",
    "    raise ValueError(f\"brain {brain} not an entry in brain2paths in axon_data.py file\")\n",
    "\n",
    "if f\"{dataset_to_save}_info\" not in brain2paths[\n",
    "    brain\n",
    "].keys() or dataset_to_save not in [\"train\", \"val\"]:\n",
    "    raise ValueError(f\"{dataset_to_save}_info not in brain2paths[{brain}].keys()\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if brain in brain2centers.keys():\n",
    "    centers_train = brain2centers[brain][0]\n",
    "    centers_val = brain2centers[brain][1]  # annotate z slice 25, 50 and 75\n",
    "    print(f\"{len(centers_train)} training samples, {len(centers_val)} val samples\")\n",
    "elif \"val_info\" in brain2paths[brain].keys():\n",
    "    centers_val = json_to_points(brain2paths[brain][\"val_info\"][\"url\"])[\n",
    "        brain2paths[brain][\"val_info\"][\"layer\"]\n",
    "    ]\n",
    "    print(f\"{len(centers_val)} val samples\")\n",
    "\n",
    "\n",
    "if \"train_info\" in brain2paths[brain].keys():\n",
    "    centers_train = json_to_points(brain2paths[brain][\"train_info\"][\"url\"])[\n",
    "        brain2paths[brain][\"train_info\"][\"layer\"]\n",
    "    ]\n",
    "    print(f\"{len(centers_train)} train samples\")\n",
    "\n",
    "if dataset_to_save == \"train\":\n",
    "    centers = centers_train\n",
    "elif dataset_to_save == \"val\":\n",
    "    centers = centers_val\n",
    "\n",
    "\n",
    "mip = 0\n",
    "\n",
    "if \"base\" in brain2paths[brain].keys():\n",
    "    base_dir_s3 = brain2paths[brain][\"base\"]\n",
    "    dir = base_dir_s3 + antibody_layer\n",
    "    vol_fg = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "    print(f\"fg shape: {vol_fg.shape} at {vol_fg.resolution}\")\n",
    "    dir = base_dir_s3 + background_layer\n",
    "    vol_bg = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "    print(f\"bg shape: {vol_bg.shape} at {vol_bg.resolution}\")\n",
    "    dir = base_dir_s3 + endogenous_layer\n",
    "    vol_endo = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "    print(f\"endo shape: {vol_endo.shape} at {vol_endo.resolution}\")\n",
    "\n",
    "    dir = base_dir_s3 + \"axon_mask\"\n",
    "    try:\n",
    "        vol_mask = CloudVolume(dir, parallel=1, mip=mip, fill_missing=True)\n",
    "    except:\n",
    "        print(\"vol_mask not found\")\n",
    "else:\n",
    "    dir = brain2paths[brain][\"ab\"]\n",
    "    vol_fg = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "    print(f\"fg shape: {vol_fg.shape} at {vol_fg.resolution}\")\n",
    "    dir = brain2paths[brain][\"bg\"]\n",
    "    vol_bg = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "    print(f\"bg shape: {vol_bg.shape} at {vol_bg.resolution}\")\n",
    "    dir = brain2paths[brain][\"endo\"]\n",
    "    vol_endo = CloudVolume(dir, parallel=1, mip=mip, fill_missing=False)\n",
    "    print(f\"endo shape: {vol_endo.shape} at {vol_endo.resolution}\")\n",
    "\n",
    "    if \"mask\" in brain2paths[brain].keys():\n",
    "        dir = brain2paths[brain][\"mask\"]\n",
    "        try:\n",
    "            vol_mask = CloudVolume(dir, parallel=1, mip=mip, fill_missing=True)\n",
    "        except:\n",
    "            print(\"vol_mask not found\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ilastik - blue/1 is axon yellow/0 is bg\n",
    "# prediction model is /Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/axon_segmentation.ilp\n",
    "\n",
    "\n",
    "isExist = os.path.exists(base_dir)\n",
    "if not isExist:\n",
    "    print(f\"Creating directory: {base_dir}\")\n",
    "    os.makedirs(base_dir)\n",
    "else:\n",
    "    print(f\"Downloaded data will be stored in {base_dir}\")\n",
    "\n",
    "if dataset_to_save == \"train\":\n",
    "    centers = centers_train\n",
    "elif dataset_to_save == \"val\":\n",
    "    centers = centers_val\n",
    "\n",
    "for i, center in enumerate(centers):\n",
    "    print(f\"Downdloading subvolumes around: {center}\")\n",
    "    image_fg = vol_fg[\n",
    "        center[0] - 49 : center[0] + 50,\n",
    "        center[1] - 49 : center[1] + 50,\n",
    "        center[2] - 49 : center[2] + 50,\n",
    "    ]\n",
    "    image_fg = image_fg[:, :, :, 0]\n",
    "\n",
    "    image_bg = vol_bg[\n",
    "        center[0] - 49 : center[0] + 50,\n",
    "        center[1] - 49 : center[1] + 50,\n",
    "        center[2] - 49 : center[2] + 50,\n",
    "    ]\n",
    "    image_bg = image_bg[:, :, :, 0]\n",
    "\n",
    "    image_endo = vol_endo[\n",
    "        center[0] - 49 : center[0] + 50,\n",
    "        center[1] - 49 : center[1] + 50,\n",
    "        center[2] - 49 : center[2] + 50,\n",
    "    ]\n",
    "    image_endo = image_endo[:, :, :, 0]\n",
    "\n",
    "    image_2channel = np.stack([image_bg, image_fg, image_endo], axis=0)\n",
    "\n",
    "    fname = f\"{base_dir}{dataset_to_save}_{int(center[0])}_{int(center[1])}_{int(center[2])}.h5\"\n",
    "    with h5py.File(fname, \"w\") as f:\n",
    "        dset = f.create_dataset(\"image_2channel\", data=image_2channel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View downloaded data (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/brain8649/val_775_4829_2459.h5\"  # path to file for viewing\n",
    "scale = [1.8, 1.8, 2]  # voxel size in microns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname, \"r\") as f:\n",
    "    pred = f.get(\"image_2channel\")\n",
    "    image_bg = pred[0, :, :, :]\n",
    "    image_fg = pred[1, :, :, :]\n",
    "    image_endo = pred[2, :, :, :]\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(image_fg, scale=scale)\n",
    "viewer.add_image(image_bg, scale=scale)\n",
    "viewer.add_image(image_endo, scale=scale)\n",
    "viewer.scale_bar.visible = True\n",
    "viewer.scale_bar.unit = \"um\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Ilastik to validation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to do two things:\n",
    "- add annotations to the downloaded data (for me, partial labels on 3 of the z-slices using ilastik)\n",
    "- apply axon segmentation model to the downloaded data. Results should be located in the same directory at the subvolumes, with the addition of \"_Probabilities\" appended to the file names. You can use `axon_apply_ilastik.py` to help apply ilastik programmatically.\n",
    "\n",
    "Note: make sure foreground/background labels are matched between the model and your annotations (for me, blue/1 =axon yellow/0=bg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 0.02\n",
    "thresholds = np.arange(spacing, 1.0, spacing)\n",
    "precisions = []\n",
    "recalls = []\n",
    "best_fscore = 0\n",
    "\n",
    "files = os.listdir(base_dir)\n",
    "files = [base_dir + f for f in files if \"val\" in f]\n",
    "files = [f for f in files if \"_Probabilities.h5\" in f]\n",
    "print(f\"{len(files)} total validation subvolumes\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    true_pos_total = 0\n",
    "    false_pos_total = 0\n",
    "    true_labels_total = 0\n",
    "    true_labels_total_neg = 0\n",
    "    for fname_prob in files:\n",
    "\n",
    "        fname_im = fname_prob[:-17] + \".h5\"\n",
    "        f = h5py.File(fname_im, \"r\")\n",
    "        im = f.get(\"image_2channel\")\n",
    "        im_bg = im[0, :, :, :]\n",
    "        im_fg = im[1, :, :, :]\n",
    "\n",
    "        fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "        f = h5py.File(fname_lab, \"r\")\n",
    "        gt = f.get(\"exported_data\")\n",
    "        gt = gt[0, :, :, :]\n",
    "        pos_labels = gt == 2\n",
    "        neg_labels = gt == 1\n",
    "\n",
    "        f = h5py.File(fname_prob, \"r\")\n",
    "        seg = f.get(\"exported_data\")\n",
    "        seg = seg[1, :, :, :]\n",
    "        mask = seg > threshold\n",
    "\n",
    "        true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "        true_pos_total += true_pos\n",
    "        false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "        false_pos_total += false_pos\n",
    "        true_labels = np.sum(pos_labels)\n",
    "        true_labels_total += true_labels\n",
    "        true_labels_neg = np.sum(neg_labels)\n",
    "        true_labels_total_neg += true_labels_neg\n",
    "\n",
    "    precision_total = true_pos_total / (true_pos_total + false_pos_total)\n",
    "    recall_total = true_pos_total / true_labels_total\n",
    "    fscore = 2 / (1 / precision_total + 1 / recall_total)\n",
    "    print(\n",
    "        f\"Thresh: {threshold:.2f} --- Total prec.: {precision_total:.3f} total rec.: {recall_total:.3f} w/{true_labels_total}/{true_labels_total_neg} total pos/neg voxels. F-score: {fscore:.4f}\"\n",
    "    )\n",
    "    if fscore > best_fscore:\n",
    "        best_fscore = fscore\n",
    "        best_prec = precision_total\n",
    "        best_recall = recall_total\n",
    "        best_threshold = threshold\n",
    "    precisions.append(precision_total)\n",
    "    recalls.append(recall_total)\n",
    "plt.plot(recalls, precisions, label=\"Prec-Rec Curve\")\n",
    "plt.scatter(\n",
    "    [best_recall],\n",
    "    [best_prec],\n",
    "    c=\"red\",\n",
    "    label=f\"Best F-score: {best_fscore:.3f} (thresh {best_threshold:.2f})\",\n",
    ")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.title(f\"Validation Brain {brain} w/{true_labels_total} Total Pos. Voxels\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If results above are not adequate improve the model and try again\n",
    "\n",
    "In my case, I identify more subvolumes from the sample at hand using the same process as for validation data, and add it as training data to the model and retrain.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(base_dir)\n",
    "files = [base_dir + f for f in files if \"val\" in f]\n",
    "files = [f for f in files if \"_Probabilities.h5\" in f]\n",
    "print(f\"{len(files)} total validation subvolumes\")\n",
    "\n",
    "\n",
    "for i, fname_prob in enumerate(files):\n",
    "    fname_im = fname_prob[:-17] + \".h5\"\n",
    "    f = h5py.File(fname_im, \"r\")\n",
    "    im = f.get(\"image_2channel\")\n",
    "    im_bg = im[0, :, :, :]\n",
    "    im_fg = im[1, :, :, :]\n",
    "    im_endo = im[2, :, :, :]\n",
    "\n",
    "    fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "    f = h5py.File(fname_lab, \"r\")\n",
    "    gt = f.get(\"exported_data\")\n",
    "    gt = gt[0, :, :, :]\n",
    "    pos_labels = gt == 2\n",
    "    neg_labels = gt == 1\n",
    "\n",
    "    f = h5py.File(fname_prob, \"r\")\n",
    "    seg = f.get(\"exported_data\")\n",
    "    seg = seg[1, :, :, :]\n",
    "    mask = seg > best_threshold\n",
    "\n",
    "    true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "    false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "    true_labels = np.sum(pos_labels)\n",
    "    true_labels_neg = np.sum(neg_labels)\n",
    "\n",
    "    if true_labels == 0:\n",
    "        recall = 1\n",
    "    else:\n",
    "        recall = true_pos / true_labels\n",
    "\n",
    "    if true_pos + false_pos == 0:\n",
    "        precision = 1\n",
    "    else:\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "\n",
    "    if precision < 0.8 or recall < 0.8:\n",
    "        print(f\"{i}) {fname_prob}: prec{precision} recall: {recall}\")\n",
    "        viewer = napari.Viewer(ndisplay=3)\n",
    "        viewer.add_image(im_fg, name=f\"fg {i}\")\n",
    "        viewer.add_image(im_bg, name=\"bg\")\n",
    "        viewer.add_image(im_endo, name=\"endo\")\n",
    "        viewer.add_labels(mask, name=\"mask\")\n",
    "        viewer.add_labels(pos_labels + 2 * neg_labels, name=\"pos labels\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figure for all validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains = [\"8650\", \"8649\", \"8613\", \"8589\", \"8590\", \"8788\"]\n",
    "\n",
    "brain_ids = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_fscores = {}\n",
    "\n",
    "for brain_id in brains:\n",
    "\n",
    "    base_dir = (\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/brain\"\n",
    "        + brain_id\n",
    "        + \"/\"\n",
    "    )\n",
    "\n",
    "    spacing = 0.02\n",
    "    thresholds = np.arange(spacing, 1.0, spacing)\n",
    "    best_fscore = 0\n",
    "\n",
    "    files = os.listdir(base_dir)\n",
    "    files = [base_dir + f for f in files if \"val\" in f]\n",
    "    files = [f for f in files if \"_Probabilities.h5\" in f]\n",
    "    fiiles = [f for f in files if \"val\" in f]\n",
    "\n",
    "    print(f\"{len(files)} total validation subvolumes for brain {brain_id}\")\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        true_pos_total = 0\n",
    "        false_pos_total = 0\n",
    "        true_labels_total = 0\n",
    "        true_labels_total_neg = 0\n",
    "        for fname_prob in files:\n",
    "\n",
    "            fname_im = fname_prob[:-17] + \".h5\"\n",
    "            f = h5py.File(fname_im, \"r\")\n",
    "            im = f.get(\"image_2channel\")\n",
    "            im_bg = im[0, :, :, :]\n",
    "            im_fg = im[1, :, :, :]\n",
    "\n",
    "            fname_lab = fname_prob[:-17] + \"-image_2channel_Labels.h5\"\n",
    "            f = h5py.File(fname_lab, \"r\")\n",
    "            gt = f.get(\"exported_data\")\n",
    "            gt = gt[0, :, :, :]\n",
    "            pos_labels = gt == 2\n",
    "            neg_labels = gt == 1\n",
    "\n",
    "            f = h5py.File(fname_prob, \"r\")\n",
    "            seg = f.get(\"exported_data\")\n",
    "            seg = seg[1, :, :, :]\n",
    "            mask = seg > threshold\n",
    "\n",
    "            true_pos = np.sum(np.logical_and(mask, pos_labels))\n",
    "            true_pos_total += true_pos\n",
    "            false_pos = np.sum(np.logical_and(mask, gt == 1))\n",
    "            false_pos_total += false_pos\n",
    "            true_labels = np.sum(pos_labels)\n",
    "            true_labels_total += true_labels\n",
    "            true_labels_neg = np.sum(neg_labels)\n",
    "            true_labels_total_neg += true_labels_neg\n",
    "\n",
    "        precision_total = true_pos_total / (true_pos_total + false_pos_total)\n",
    "        recall_total = true_pos_total / true_labels_total\n",
    "\n",
    "        precisions.append(precision_total)\n",
    "        recalls.append(recall_total)\n",
    "        brain_ids.append(brain_id)\n",
    "\n",
    "        fscore = 2 / (1 / precision_total + 1 / recall_total)\n",
    "\n",
    "        if fscore > best_fscore:\n",
    "            best_fscore = fscore\n",
    "            best_prec = precision_total\n",
    "            best_recall = recall_total\n",
    "            best_threshold = threshold\n",
    "    best_precisions.append(best_prec)\n",
    "    best_recalls.append(best_recall)\n",
    "    best_fscores[brain_id] = best_fscore\n",
    "for i, brain_id in enumerate(brain_ids):\n",
    "    brain_ids[i] = brain_id + f\" - Max F-score: {best_fscores[brain_id]:.2f}\"\n",
    "\n",
    "data = {\"Sample\": brain_ids, \"Recall\": recalls, \"Precision\": precisions}\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (10, 7)})\n",
    "sns.set(font_scale=2)\n",
    "sns.lineplot(data=df, x=\"Recall\", y=\"Precision\", hue=\"Sample\")\n",
    "sns.scatterplot(x=best_recalls, y=best_precisions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make annotation layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axon segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"base\" in brain2paths[brain].keys():\n",
    "    dir = brain2paths[brain][\"base\"] + \"axon_mask\"\n",
    "else:\n",
    "    dir = brain2paths[brain][\"mask\"]\n",
    "\n",
    "try:\n",
    "    CloudVolume(dir)\n",
    "except:\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels=1,\n",
    "        layer_type=\"segmentation\",\n",
    "        data_type=\"uint64\",  # Channel images might be 'uint8'\n",
    "        encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "        resolution=vol_bg.resolution,  # Voxel scaling, units are in nanometers\n",
    "        voxel_offset=vol_bg.voxel_offset,  # x,y,z offset in voxels from the origin\n",
    "        # mesh            = 'mesh',\n",
    "        # Pick a convenient size for your underlying chunk representation\n",
    "        # Powers of two are recommended, doesn't need to cover image exactly\n",
    "        chunk_size=[128, 128, 2],  # units are voxels\n",
    "        volume_size=vol_bg.volume_size,  # e.g. a cubic millimeter dataset\n",
    "    )\n",
    "    vol_mask = CloudVolume(dir, info=info)\n",
    "    vol_mask.commit_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_vol = CloudVolume(\n",
    "    \"precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017\"\n",
    ")\n",
    "for layer in [\n",
    "    antibody_layer,\n",
    "    background_layer,\n",
    "    \"axon_mask\",\n",
    "]:  # axon_mask is transformed into an image because nearest interpolation doesnt work well after downsampling\n",
    "\n",
    "    layer_path = brain2paths[brain][\"base\"] + layer + \"_transformed\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels=1,\n",
    "        layer_type=\"image\",\n",
    "        data_type=\"uint16\",  # Channel images might be 'uint8'\n",
    "        encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "        resolution=atlas_vol.resolution,  # Voxel scaling, units are in nanometers\n",
    "        voxel_offset=atlas_vol.voxel_offset,\n",
    "        chunk_size=[32, 32, 32],  # units are voxels\n",
    "        volume_size=atlas_vol.volume_size,  # e.g. a cubic millimeter dataset\n",
    "    )\n",
    "    vol_mask = CloudVolume(layer_path, info=info)\n",
    "    vol_mask.commit_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply ilastik to whole image:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. using `axon_segment_image.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Register volume using CloudReg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. You need to find an initial affine alignment using cloudreg.scripts.registration.get_affine_matrix. For example: \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from cloudreg.scripts.registration import get_affine_matrix\n",
    "get_affine_matrix([1,1,1], [15,0,0], \"PIR\", \"RAI\", 1.15, \"precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Run registration using cloudreg.scripts.registration. For example:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python -m cloudreg.scripts.registration -input_s3_path precomputed://s3://smartspim-precomputed-volumes/2022_11_01/8790/Ch_561 --output_s3_path precomputed://s3://smartspim-precomputed-volumes/2022_11_01/8790/atlas_to_target --atlas_s3_path https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_50um/average_50um --parcellation_s3_path https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017 --atlas_orientation PIR -orientation RAI --rotation 0 0 0 --translation 0 0 0 --fixed_scale 1.2 -log_s3_path precomputed://s3://smartspim-precomputed-volumes/2022_11_01/8790/atlas_to_target --missing_data_correction True --grid_correction False --bias_correction True --regularization 5000.0 --iterations 3000 --registration_resolution 100\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Transform segmentation to atlas space using CloudReg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python -m cloudreg.scripts.transform_data --target_layer_source precomputed://s3://smartspim-precomputed-volumes/2022_11_03/8589/axon_mask --transformed_layer_source precomputed://s3://smartspim-precomputed-volumes/2022_11_03/8589/axon_mask_transformed --affine_path /mnt/NAS/Neuroglancer\\ Data/2021_11_03/8589/8589_Ch_561_registration/downloop_1_A.mat  --velocity_path /mnt/NAS/Neuroglancer\\ Data/2021_11_03/8589/8589_Ch_561_registration/downloop_1_v.mat\n",
    "```\n",
    "\n",
    "This will write a layer to s3 with the transformed axon mask. The s3 path to this layer should be added to `axon_data.py` under the `axon_mask_transformed` key. Then the code below, or `axon_brainrender.py`, can be used to visualize the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View coronal heat maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A couple useful lists of samples\n",
    "\n",
    "brain_ids = [\"8650\", \"8649\", \"8589\", \"8590\", \"8613\"]\n",
    "brain_ids_good = [\"8650\", \"8788\", \"8613\", \"8589\", \"8786\"]\n",
    "brain_ids_select = [\"8650\", \"8788\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_ids = brain_ids_select  # list of sample IDs to show results\n",
    "\n",
    "atlas_level = 5  # level of ARA to partition the figures\n",
    "\n",
    "atlas_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/ara/ara_10um.tif\"  # Path to 10um atlas parcellation\n",
    "# atlas can be downloaded from here: https://neurodata.io/data/allen_atlas/\n",
    "fold_on = True  # Whether to fold results over midline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = setup_atlas_graph()\n",
    "atlas_level_nodes = get_atlas_level_nodes(atlas_level, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type2id = {}\n",
    "for brain_id in brain_ids:\n",
    "    genotype = brain2paths[brain_id][\"genotype\"]\n",
    "    if genotype not in type2id.keys():\n",
    "        type2id[genotype] = [brain_id]\n",
    "    else:\n",
    "        new_val = type2id[genotype] + [brain_id]\n",
    "        type2id[genotype] = new_val\n",
    "\n",
    "\n",
    "vols_transformed_gad = [\n",
    "    CloudVolume(brain2paths[id][\"base\"] + \"axon_mask_transformed\")\n",
    "    if \"base\" in brain2paths[id].keys()\n",
    "    else CloudVolume(brain2paths[id][\"transformed_mask\"])\n",
    "    for id in type2id[\"tph2 gad2\"]\n",
    "]\n",
    "vols_transformed_vglut = [\n",
    "    CloudVolume(brain2paths[id][\"base\"] + \"axon_mask_transformed\")\n",
    "    if \"base\" in brain2paths[id].keys()\n",
    "    else CloudVolume(brain2paths[id][\"transformed_mask\"])\n",
    "    for id in type2id[\"tph2 vglut3\"]\n",
    "]\n",
    "\n",
    "atlas = io.imread(atlas_path)\n",
    "print(f\"Shape of atlas image: {atlas.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = {}\n",
    "for z in np.arange(100, 1300, 700):\n",
    "    slice = atlas[z, :, :]\n",
    "    newslice = np.copy(slice)\n",
    "    for label in tqdm(np.unique(slice), desc=f\"Relabeling in z={z}\"):\n",
    "        atlas_level_label = find_atlas_level_label(\n",
    "            label, atlas_level_nodes, atlas_level, G\n",
    "        )\n",
    "        newslice[slice == label] = atlas_level_label\n",
    "        if atlas_level_label not in new_labels.keys():\n",
    "            if atlas_level_label in G.nodes:\n",
    "                name = G.nodes[atlas_level_label][\"name\"]\n",
    "            else:\n",
    "                name = \"??\"\n",
    "            new_labels[atlas_level_label] = name\n",
    "\n",
    "    print(new_labels)\n",
    "\n",
    "    labels = measure.label(newslice)\n",
    "    borders = 0 * labels\n",
    "    for label in tqdm(np.unique(labels), desc=f\"Processing labels in z={z}\"):\n",
    "        if label != 0:\n",
    "            mask = np.array(labels == label, dtype=\"int\")\n",
    "            erode = np.array(ndi.binary_erosion(mask))\n",
    "            outline = mask - erode\n",
    "            borders += outline\n",
    "\n",
    "    v = napari.Viewer()\n",
    "    # v.add_labels(newslice, scale = [10,10])\n",
    "\n",
    "    # get projection data\n",
    "    for volnum, vol_transformed_gad in enumerate(vols_transformed_gad):\n",
    "        slice_gad = np.squeeze(vol_transformed_gad[z - 10 : z + 10, :, :])\n",
    "        slice_gad = ndi.gaussian_filter(slice_gad.astype(float), sigma=(3, 3, 3))[\n",
    "            10, :, :\n",
    "        ]\n",
    "        slice_gad[newslice == 0] = 0\n",
    "        if volnum == 0:\n",
    "            slice_gad_total = slice_gad\n",
    "        else:\n",
    "            slice_gad_total += slice_gad\n",
    "\n",
    "    for volnum, vol_transformed_vglut in enumerate(vols_transformed_vglut):\n",
    "        slice_vglut = np.squeeze(vol_transformed_vglut[z - 10 : z + 10, :, :])\n",
    "        slice_vglut = ndi.gaussian_filter(slice_vglut.astype(float), sigma=(3, 3, 3))[\n",
    "            10, :, :\n",
    "        ]\n",
    "        slice_vglut[newslice == 0] = 0\n",
    "        if volnum == 0:\n",
    "            slice_vglut_total = slice_vglut\n",
    "        else:\n",
    "            slice_vglut_total += slice_vglut\n",
    "\n",
    "    # only show hemisphere\n",
    "    slice_vglut_total /= np.amax(slice_vglut_total)\n",
    "    slice_gad_total /= np.amax(slice_gad_total)\n",
    "\n",
    "    if fold_on:\n",
    "        slice_gad_total = fold(slice_gad_total)\n",
    "        slice_vglut_total = fold(slice_vglut_total)\n",
    "        borders = borders[:, : int(borders.shape[1] / 2)]\n",
    "\n",
    "    projections = np.stack(\n",
    "        [slice_gad_total, slice_vglut_total, 0 * slice_vglut_total], axis=-1\n",
    "    )\n",
    "    v.add_image(projections, rgb=True, scale=[10, 10], name=f\"vglut=green,gad2=red\")\n",
    "    v.add_labels(borders * 2, scale=[10, 10], name=f\"z={z}\")\n",
    "    v.scale_bar.unit = \"um\"\n",
    "    v.scale_bar.visible = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Collect region based results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `axon_collect_results.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Display bar charts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebrain_results_dir = \"\"  #\n",
    "\n",
    "brains = [\n",
    "    # \"3\",\n",
    "    # \"4\",\n",
    "    \"8613\",\n",
    "    # \"8604\",\n",
    "    \"8650\",\n",
    "    \"8589\",\n",
    "    # \"8590\",\n",
    "    # \"8649\",\n",
    "    \"8788\",\n",
    "    \"8786\",\n",
    "    \"11537\",\n",
    "    \"8790\",\n",
    "]  # list of sample IDs to be shown\n",
    "\n",
    "regions = [\n",
    "    688,  # cerebral cortex\n",
    "    698,  # olfactory areas\n",
    "    1089,  # hippocampal formation\n",
    "    # 583, # claustrum\n",
    "    477,  # striatum\n",
    "    # 803, # pallidum\n",
    "    351,  # bed nuclei of stria terminalis\n",
    "    # 703, #cortical subplate\n",
    "    1097,  # hypothalamus\n",
    "    549,  # thalamus\n",
    "    186,  # lateral habenula\n",
    "    519,  # cerebellar nuclei\n",
    "    313,  # midbrain\n",
    "    1065,  # hindbrain\n",
    "]  # allen atlas region IDs to be shown\n",
    "# see: https://connectivity.brain-map.org/projection/experiment/480074702?imageId=480075280&initImage=TWO_PHOTON&x=17028&y=11704&z=3\n",
    "\n",
    "composite_regions = {\n",
    "    \"Amygdalar Nuclei\": [131, 295, 319, 780]\n",
    "}  # Custom composite allen regions where key is region name and value is list of allen regions\n",
    "\n",
    "\n",
    "level = \"coarse\"  # coarse or fine, dictates whether the regions specified above will be shown (coarse), or their subregions (fine)\n",
    "\n",
    "if level not in [\"coarse\", \"fine\"]:\n",
    "    raise ValueError(f\"level must be coarse or fine, not {level}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "genotypes = []\n",
    "for brain_id in brains:\n",
    "    genotypes.append(brain2paths[brain_id][\"genotype\"])\n",
    "for unq_gene in set(genotypes):\n",
    "    count = 0\n",
    "    for genotype in genotypes:\n",
    "        if genotype == unq_gene:\n",
    "            count += 1\n",
    "    counts[unq_gene] = count\n",
    "\n",
    "\n",
    "quantification_dicts = {}\n",
    "\n",
    "for brain in brains:\n",
    "    path = (\n",
    "        \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/wholebrain_results/wholebrain_\"\n",
    "        + brain\n",
    "        + \".pkl\"\n",
    "    )\n",
    "    with open(path, \"rb\") as f:\n",
    "        quantification_dict = pickle.load(f)\n",
    "\n",
    "    quantification_dicts[brain] = quantification_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = json.load(\n",
    "    open(\n",
    "        brainlit_path\n",
    "        / \"brainlit\"\n",
    "        / \"lsm_analysis\"\n",
    "        / \"data\"\n",
    "        / \"ara_structure_ontology.json\",\n",
    "        \"r\",\n",
    "    )\n",
    ")\n",
    "\n",
    "tree = build_tree(f)\n",
    "stack = [tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [tree]\n",
    "cur_level = -1\n",
    "counter = 0\n",
    "G = nx.DiGraph()\n",
    "max_level = 0\n",
    "\n",
    "# Initialize nodes\n",
    "\n",
    "while len(queue) > 0:\n",
    "    node = queue.pop(0)\n",
    "    if node.level > max_level:\n",
    "        max_level = node.level\n",
    "    G.add_node(\n",
    "        node.id,\n",
    "        level=node.level,\n",
    "        st_level=node.st_level,\n",
    "        name=node.name,\n",
    "        acronym=node.acronym,\n",
    "        label=str(node.st_level) + \") \" + node.name,\n",
    "    )\n",
    "    for brain in quantification_dicts.keys():\n",
    "        G.nodes[node.id][brain + \" axon\"] = 0\n",
    "        G.nodes[node.id][brain + \" total\"] = 0\n",
    "    if node.parent_id is not None:\n",
    "        G.add_edge(node.parent_id, node.id)\n",
    "\n",
    "    queue += node.children\n",
    "\n",
    "# add data\n",
    "i_test = 0\n",
    "print(f\"Max level: {max_level}\")\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" axon\"])\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" total\"])\n",
    "for brain, quantification_dict in quantification_dicts.items():\n",
    "    for key in quantification_dict.keys():\n",
    "        if key in G.nodes:\n",
    "            G.nodes[key][brain + \" axon\"] = G.nodes[key][brain + \" axon\"] + float(\n",
    "                quantification_dict[key][1]\n",
    "            )\n",
    "            G.nodes[key][brain + \" total\"] = G.nodes[key][brain + \" total\"] + float(\n",
    "                quantification_dict[key][0]\n",
    "            )\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" axon\"])\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" total\"])\n",
    "\n",
    "# add child data to parent data\n",
    "for brain in quantification_dicts.keys():\n",
    "    for lvl in range(max_level, 0, -1):\n",
    "        for node in G.nodes:\n",
    "            if G.nodes[node][\"level\"] == lvl:\n",
    "                parent = list(G.in_edges(node))[0][0]\n",
    "                G.nodes[parent][brain + \" axon\"] = (\n",
    "                    G.nodes[parent][brain + \" axon\"] + G.nodes[node][brain + \" axon\"]\n",
    "                )\n",
    "                G.nodes[parent][brain + \" total\"] = (\n",
    "                    G.nodes[parent][brain + \" total\"] + G.nodes[node][brain + \" total\"]\n",
    "                )\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" axon\"])\n",
    "print(G.nodes[997][list(quantification_dicts.keys())[i_test] + \" total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = {}\n",
    "\n",
    "for brain in quantification_dicts.keys():\n",
    "    total = 0\n",
    "    for node in G.nodes:\n",
    "        total += G.nodes[node][brain + \" axon\"]\n",
    "    totals[brain] = total\n",
    "\n",
    "axon_vols = []\n",
    "axon_denss = []\n",
    "axon_denss_norm = []\n",
    "gene = []\n",
    "subregion_name = []\n",
    "region_name = []\n",
    "brain_ids = []\n",
    "for region in regions:\n",
    "    print(f\"Populating: \" + G.nodes[region][\"name\"])\n",
    "    # choose level here\n",
    "    if level == \"fine\":\n",
    "        children = list(G.successors(region))\n",
    "    elif level == \"coarse\":\n",
    "        children = [region]\n",
    "    else:\n",
    "        raise ValueError(f\"level must be coarse or fine, not {level}\")\n",
    "\n",
    "    for child in children:\n",
    "        for brain in quantification_dicts.keys():\n",
    "            drn_norm_factor = (\n",
    "                G.nodes[872][brain + \" axon\"] / G.nodes[872][brain + \" total\"]\n",
    "            )\n",
    "\n",
    "            if drn_norm_factor == 0:\n",
    "                print(f\"Warning: brain {brain} has no projection in DRN\")\n",
    "                drn_norm_factor = 1\n",
    "\n",
    "            axon_vol = G.nodes[child][brain + \" axon\"]\n",
    "            total_vol = G.nodes[child][brain + \" total\"]\n",
    "            if total_vol == 0 and axon_vol == 0:\n",
    "                axon_denss.append(0)\n",
    "                axon_denss_norm.append(0)\n",
    "            elif total_vol == 0:\n",
    "                raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "            else:\n",
    "                dens = axon_vol / total_vol\n",
    "                axon_denss.append(dens * 100)\n",
    "                axon_denss_norm.append(dens / drn_norm_factor)\n",
    "\n",
    "            axon_vols.append(axon_vol / totals[brain] * 100)\n",
    "            gene.append(\n",
    "                brain2paths[brain][\"genotype\"]\n",
    "                + f\" (n={counts[brain2paths[brain]['genotype']]})\"\n",
    "            )\n",
    "            subregion_name.append(G.nodes[child][\"name\"])\n",
    "            region_name.append(G.nodes[region][\"name\"])\n",
    "            brain_ids.append(brain)\n",
    "\n",
    "for region_component_name in composite_regions.keys():\n",
    "    print(f\"Populating: \" + region_component_name)\n",
    "    region_components = composite_regions[region_component_name]\n",
    "    for brain in quantification_dicts.keys():\n",
    "        drn_norm_factor = G.nodes[872][brain + \" axon\"] / G.nodes[872][brain + \" total\"]\n",
    "        if drn_norm_factor == 0:\n",
    "            print(f\"Warning: brain {brain} has no projection in DRN\")\n",
    "            drn_norm_factor = 1\n",
    "\n",
    "        axon_vol = 0\n",
    "        total_vol = 0\n",
    "\n",
    "        for region_component in region_components:\n",
    "            axon_vol += G.nodes[region_component][brain + \" axon\"]\n",
    "            total_vol += G.nodes[region_component][brain + \" total\"]\n",
    "\n",
    "        if total_vol == 0 and axon_vol == 0:\n",
    "            axon_denss.append(0)\n",
    "            axon_denss_norm.append(0)\n",
    "        elif total_vol == 0:\n",
    "            raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "        else:\n",
    "            dens = axon_vol / total_vol\n",
    "            axon_denss.append(dens * 100)\n",
    "            axon_denss_norm.append(dens / drn_norm_factor)\n",
    "\n",
    "        axon_vols.append(axon_vol / totals[brain] * 100)\n",
    "        gene.append(\n",
    "            brain2paths[brain][\"genotype\"]\n",
    "            + f\" (n={counts[brain2paths[brain]['genotype']]})\"\n",
    "        )\n",
    "        subregion_name.append(region_component_name)\n",
    "        region_name.append(region_component_name)\n",
    "        brain_ids.append(brain)\n",
    "\n",
    "\n",
    "d = {\n",
    "    \"Percent Total Axon Volume (%)\": axon_vols,\n",
    "    \"Axon Density (%)\": axon_denss,\n",
    "    \"Normalized Axon Density\": axon_denss_norm,\n",
    "    \"Gene\": gene,\n",
    "    \"Subregion\": subregion_name,\n",
    "    \"Region\": region_name,\n",
    "    \"Brain ID\": brain_ids,\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(26, 13))\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "test = \"Mann-Whitney\"\n",
    "# test = \"t-test_ind\"\n",
    "correction = \"fdr_by\"\n",
    "\n",
    "# density\n",
    "fig_args = {\n",
    "    \"y\": \"Axon Density (%)\",\n",
    "    \"x\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "pairs = []\n",
    "unq_subregions = []\n",
    "for subregion in subregion_name:\n",
    "    if subregion not in unq_subregions:\n",
    "        unq_subregions.append(subregion)\n",
    "\n",
    "\n",
    "genes = df[\"Gene\"].unique()\n",
    "gene_pairs = [(a, b) for idx, a in enumerate(genes) for b in genes[idx + 1 :]]\n",
    "\n",
    "for gene_pair in gene_pairs:\n",
    "    for subregion in unq_subregions:\n",
    "        pairs.append(\n",
    "            (\n",
    "                (subregion, gene_pair[0]),\n",
    "                (subregion, gene_pair[1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "annotator = Annotator(axes[0], pairs, **fig_args)\n",
    "fig_args = {\n",
    "    \"x\": \"Axon Density (%)\",\n",
    "    \"y\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "strpplot = sns.barplot(ax=axes[0], orient=\"h\", **fig_args)\n",
    "strpplot.set_xscale(\"log\")\n",
    "\n",
    "annotator.configure(\n",
    "    test=test, text_format=\"star\", loc=\"outside\", comparisons_correction=correction\n",
    ")\n",
    "annotator.new_plot(strpplot, orient=\"h\", plot=\"barplot\", **fig_args)\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "# percent total\n",
    "fig_args = {\n",
    "    \"y\": \"Percent Total Axon Volume (%)\",\n",
    "    \"x\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "pairs = []\n",
    "unq_subregions = []\n",
    "for subregion in subregion_name:\n",
    "    if subregion not in unq_subregions:\n",
    "        unq_subregions.append(subregion)\n",
    "\n",
    "\n",
    "for gene_pair in gene_pairs:\n",
    "    for subregion in unq_subregions:\n",
    "        pairs.append(\n",
    "            (\n",
    "                (subregion, gene_pair[0]),\n",
    "                (subregion, gene_pair[1]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "annotator = Annotator(axes[1], pairs, **fig_args)\n",
    "fig_args = {\n",
    "    \"x\": \"Percent Total Axon Volume (%)\",\n",
    "    \"y\": \"Subregion\",\n",
    "    \"hue\": \"Gene\",\n",
    "    \"data\": df,\n",
    "}\n",
    "\n",
    "strpplot = sns.barplot(ax=axes[1], orient=\"h\", **fig_args)\n",
    "strpplot.set_xscale(\"log\")\n",
    "\n",
    "annotator.configure(\n",
    "    test=test, text_format=\"star\", loc=\"outside\", comparisons_correction=correction\n",
    ")\n",
    "annotator.new_plot(strpplot, orient=\"h\", plot=\"barplot\", **fig_args)\n",
    "annotator.apply_and_annotate()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare distributions with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "brain_ids = []\n",
    "genotypes = []\n",
    "\n",
    "for i, brain in enumerate(brains):\n",
    "    print(brain)\n",
    "    region_order = list(df.loc[df[\"Brain ID\"] == brain][\"Region\"])\n",
    "\n",
    "    if i == 0:\n",
    "        standard_region_order = region_order\n",
    "    elif standard_region_order != region_order:\n",
    "        raise ValueError(f\"Different region order for brain {brain}\")\n",
    "\n",
    "    distrib = list(df.loc[df[\"Brain ID\"] == brain][\"Percent Total Axon Volume (%)\"])\n",
    "    X.append(distrib)\n",
    "\n",
    "    brain_ids.append(brain)\n",
    "    genotypes.append(brains[brain])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2 = pca.fit_transform(X)\n",
    "\n",
    "df_pca = {\n",
    "    \"PC 1\": X_2[:, 0],\n",
    "    \"PC 2\": X_2[:, 1],\n",
    "    \"Genotype\": genotypes,\n",
    "    \"Brain ID\": brain_ids,\n",
    "}\n",
    "df_pca = pd.DataFrame(data=df_pca)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.scatterplot(data=df_pca, x=\"PC 1\", y=\"PC 2\", hue=\"Genotype\", ax=ax)\n",
    "\n",
    "\n",
    "for i in range(df_pca.shape[0]):\n",
    "    plt.text(\n",
    "        x=df_pca[\"PC 1\"][i] + 0.03,\n",
    "        y=df_pca[\"PC 2\"][i] + 0.03,\n",
    "        s=df_pca[\"Brain ID\"][i],\n",
    "        fontdict=dict(color=\"black\", size=20),\n",
    "    )\n",
    "\n",
    "plt.title(\n",
    "    f\"Projection Distribution PCA with Explained Variance: {pca.explained_variance_ratio_}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Compare to Allen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allen_regions = [\n",
    "    315,\n",
    "    698,\n",
    "    1089,\n",
    "    703,\n",
    "    477,\n",
    "    803,\n",
    "    549,\n",
    "    1097,\n",
    "    313,\n",
    "    771,\n",
    "    354,\n",
    "    512,\n",
    "]  # allen atlas region IDs to be shown https://connectivity.brain-map.org/projection/experiment/480074702?imageId=480075280&initImage=TWO_PHOTON&x=17028&y=11704&z=3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_denss = []\n",
    "gene = []\n",
    "subregion_name = []\n",
    "region_name = []\n",
    "subregions_list = []\n",
    "for region in allen_regions:\n",
    "    print(f\"Populating: \" + G.nodes[region][\"name\"])\n",
    "    children = list(G.successors(region))\n",
    "    for child in children:\n",
    "        if child not in subregions_list:\n",
    "            subregions_list.append(child)\n",
    "\n",
    "        for brain in quantification_dicts.keys():\n",
    "            if (\n",
    "                G.nodes[child][brain + \" total\"] == 0\n",
    "                and G.nodes[child][brain + \" axon\"] == 0\n",
    "            ):\n",
    "                axon_denss.append(0)\n",
    "            elif G.nodes[child][brain + \" total\"] == 0:\n",
    "                raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "            else:\n",
    "                axon_denss.append(\n",
    "                    G.nodes[child][brain + \" axon\"] / G.nodes[child][brain + \" total\"]\n",
    "                )\n",
    "\n",
    "            if brain in [\"B\", \"R\"]:\n",
    "                gene.append(brain)\n",
    "            subregion_name.append(G.nodes[child][\"name\"])\n",
    "\n",
    "    region_name.append(G.nodes[region][\"name\"])\n",
    "\n",
    "tree = ET.parse(brainlit_path / \"brainlit\" / \"lsm_analysis\" / \"data\" / \"sert_exp.xml\")\n",
    "root = tree.getroot()\n",
    "root.tag\n",
    "for child in root:\n",
    "    for i, entry in enumerate(child):\n",
    "        for item in entry:\n",
    "            if item.tag == \"structure-id\":\n",
    "                region = int(item.text)\n",
    "            elif item.tag == \"hemisphere-id\":\n",
    "                hemi = int(item.text)\n",
    "            elif item.tag == \"is-injection\":\n",
    "                inject = item.text\n",
    "            elif item.tag == \"projection-density\":\n",
    "                density = float(item.text)\n",
    "        if region in subregions_list and hemi == 3 and inject == \"false\":\n",
    "            name = G.nodes[region][\"name\"]\n",
    "            print(f\"id: {region} hemi: {hemi}, density: {density}, name: {name}\")\n",
    "            subregion_name.append(name)\n",
    "            gene.append(\"Allen\")\n",
    "            axon_denss.append(density)\n",
    "\n",
    "\n",
    "d = {\"Axon Density\": axon_denss, \"Gene\": gene, \"Subregion\": subregion_name}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(20, 10))\n",
    "fig.suptitle(\"Detected Output Axons\")\n",
    "\n",
    "sns.barplot(x=\"Axon Density\", y=\"Subregion\", hue=\"Gene\", data=df)\n",
    "axes.set_title(\"Density\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axon_denss = []\n",
    "axon_vols = []\n",
    "gene = []\n",
    "region_name = []\n",
    "for region in allen_regions:\n",
    "    print(f\"Populating: \" + G.nodes[region][\"name\"])\n",
    "    for brain in quantification_dicts.keys():\n",
    "        if (\n",
    "            G.nodes[region][brain + \" total\"] == 0\n",
    "            and G.nodes[region][brain + \" axon\"] == 0\n",
    "        ):\n",
    "            axon_denss.append(0)\n",
    "        elif G.nodes[region][brain + \" total\"] == 0:\n",
    "            raise ValueError(\"positive axon volume in zero volume region?\")\n",
    "        else:\n",
    "            axon_denss.append(\n",
    "                G.nodes[region][brain + \" axon\"] / G.nodes[region][brain + \" total\"]\n",
    "            )\n",
    "            axon_vols.append(\n",
    "                G.nodes[region][brain + \" axon\"]\n",
    "                * np.product([1.82, 1.82, 2])\n",
    "                * 10 ** (-9)\n",
    "            )\n",
    "\n",
    "        if brain in [\"B\", \"R\"]:\n",
    "            gene.append(\"Sample \" + brain)\n",
    "\n",
    "        region_name.append(G.nodes[region][\"name\"])\n",
    "\n",
    "tree = ET.parse(brainlit_path / \"brainlit\" / \"lsm_analysis\" / \"data\" / \"sert_exp.xml\")\n",
    "root = tree.getroot()\n",
    "root.tag\n",
    "for child in root:\n",
    "    for i, entry in enumerate(child):\n",
    "        for item in entry:\n",
    "            if item.tag == \"structure-id\":\n",
    "                region = int(item.text)\n",
    "            elif item.tag == \"hemisphere-id\":\n",
    "                hemi = int(item.text)\n",
    "            elif item.tag == \"is-injection\":\n",
    "                inject = item.text\n",
    "            elif item.tag == \"projection-density\":\n",
    "                density = float(item.text)\n",
    "            elif item.tag == \"projection-volume\":\n",
    "                volume = float(item.text)\n",
    "        if region in allen_regions and hemi == 3 and inject == \"false\":\n",
    "            name = G.nodes[region][\"name\"]\n",
    "            print(\n",
    "                f\"id: {region} hemi: {hemi}, density: {density}, volume: {volume}, name: {name}\"\n",
    "            )\n",
    "            region_name.append(name)\n",
    "            gene.append(\"Allen\")\n",
    "            axon_denss.append(density)\n",
    "            axon_vols.append(volume)\n",
    "\n",
    "\n",
    "d = {\n",
    "    \"Axon Density\": axon_denss,\n",
    "    \"Axon Volume ($mm^3$)\": axon_vols,\n",
    "    \"Gene\": gene,\n",
    "    \"Region\": region_name,\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "fig.suptitle(\"Comparing Axon Volumes to Allen Experiment\")\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "sns.barplot(\n",
    "    ax=axes[0],\n",
    "    x=\"Axon Density\",\n",
    "    y=\"Region\",\n",
    "    hue=\"Gene\",\n",
    "    order=list(\n",
    "        df[df[\"Gene\"] == \"Allen\"]\n",
    "        .sort_values(\"Axon Density\", ascending=False)\n",
    "        .loc[:, \"Region\"]\n",
    "    ),\n",
    "    data=df,\n",
    ")\n",
    "# axes[0].set_title(\"Density\")\n",
    "\n",
    "sns.barplot(\n",
    "    ax=axes[1],\n",
    "    x=\"Axon Volume ($mm^3$)\",\n",
    "    y=\"Region\",\n",
    "    hue=\"Gene\",\n",
    "    order=list(\n",
    "        df[df[\"Gene\"] == \"Allen\"]\n",
    "        .sort_values(\"Axon Density\", ascending=False)\n",
    "        .loc[:, \"Region\"]\n",
    "    ),\n",
    "    data=df,\n",
    ")\n",
    "# axes[1].set_title(\"Axon Volume\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('docs_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
