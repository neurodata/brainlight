import argparse
import boto3
import paramiko
import os
from functools import partial


# assume boto3 credentials are already configured


def run_command_on_server(command, ssh_key_path, ip_address, username="ubuntu"):

    key = paramiko.RSAKey.from_private_key_file(ssh_key_path)
    client = paramiko.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

    # Connect/ssh to an instance
    try:
        # Here 'ip_address' is public IP of EC2
        client.connect(hostname=ip_address, username=username, pkey=key)

        # Execute a command after connecting/ssh to an instance
        stdin, stdout, stderr = client.exec_command(command, get_pty=True)
        for line in iter(stdout.readline, ""):
            print(line, end="")

        # output = stdout.read().decode('utf-8')
        errors = stderr.read().decode("utf-8")

        # close the client connection once the job is done
        client.close()
        return errors

    except Exception as e:
        print(e)


def run_colm_pipeline(
    ssh_key_path,
    instance_id,
    input_s3_path,
    output_s3_path,
    num_channels,
    autofluorescence_channel,
    log_s3_path=None,
    instance_type="r5d.24xlarge",
):
    # get ec2 client
    ec2 = boto3.resource("ec2")

    # stop instance in case it is running
    ec2.meta.client.stop_instances(InstanceIds=[instance_id])
    waiter = ec2.meta.client.get_waiter("instance_stopped")
    waiter.wait(InstanceIds=[instance_id])
    # make sure instance is the right type
    ec2.meta.client.modify_instance_attribute(
        InstanceId=instance_id, Attribute="instanceType", Value=instance_type
    )
    # start instance
    print("starting EC2 instance...")
    ec2.meta.client.start_instances(InstanceIds=[instance_id])
    # wait until instance is started up
    waiter = ec2.meta.client.get_waiter("instance_status_ok")
    waiter.wait(InstanceIds=[instance_id])
    print(f"{instance_type} EC2 instance started")
    # get instance ip address
    instance = ec2.Instance(instance_id)

    # now run command on instance
    # update the code on the instance
    update_command = "mkdir -p ~/ssd1 ~/ssd2; cd CloudReg; git pull;"
    print("updating CloudReg code on EC2 instance...")
    errors_update = run_command_on_server(
        update_command, ssh_key_path, instance.public_ip_address
    )
    # mount ssds command
    command1 = "sudo bash CloudReg/scripts/mount_combined_ssds.sh"
    # colm pipeline command
    command2 = f"time /home/ubuntu/colm_pipeline_env/bin/python CloudReg/scripts/colm_pipeline.py {input_s3_path} {output_s3_path} {num_channels} {autofluorescence_channel} --log_s3_path {log_s3_path}"
    print(command2)
    errors1 = run_command_on_server(command1, ssh_key_path, instance.public_ip_address)
    print(errors1)
    errors2 = run_command_on_server(command2, ssh_key_path, instance.public_ip_address)
    print(errors2)

    # shut down instance
    ec2.meta.client.stop_instances(InstanceIds=[instance_id])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        "Run COLM pipeline on remote EC2 instance with given input parameters"
    )
    parser.add_argument(
        "-ssh_key_path", help="path to identity file used to ssh into given instance"
    )
    parser.add_argument(
        "-instance_id", help="EC2 Instance ID of instance to run COLM pipeline on."
    )
    parser.add_argument(
        "-input_s3_path",
        help="S3 path to input colm data. Should be of the form s3://<bucket>/<experiment>",
        type=str,
    )
    parser.add_argument(
        "-output_s3_path",
        help="S3 path to store precomputed volume. Precomputed volumes for each channel will be stored under this path. Should be of the form s3://<bucket>/<path_to_precomputed>. The data will be saved at s3://<bucket>/<path_to_precomputed>/CHN0<channel>",
        type=str,
    )
    # parser.add_argument('channel_of_interest', help='Channel of interest in experiment',  type=int)
    parser.add_argument(
        "-num_channels", help="Number of channels in experiment", type=int
    )
    parser.add_argument(
        "-autofluorescence_channel", help="Autofluorescence channel number.", type=int
    )
    parser.add_argument(
        "-log_s3_path",
        help="S3 path at which pipeline intermediates can be stored including bias correctin tile.",
        type=str,
        default="",
    )
    parser.add_argument(
        "--instance_type",
        help="EC2 instance type to run pipeline on. minimum r5d.16xlarge",
        type=str,
        default="r5d.16xlarge",
    )

    args = parser.parse_args()

    run_colm_pipeline(
        args.ssh_key_path,
        args.instance_id,
        args.input_s3_path,
        args.output_s3_path,
        args.num_channels,
        args.autofluorescence_channel,
        args.log_s3_path,
        instance_type=args.instance_type,
    )
