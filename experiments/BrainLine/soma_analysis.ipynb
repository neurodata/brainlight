{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soma Detection Analysis of Whole-Brain Light-Sheet Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainlit.preprocessing import removeSmallCCs\n",
    "from brainlit.BrainLine.data.soma_data import brain2paths, brain2centers\n",
    "from brainlit.BrainLine.analyze_results import SomaDistribution\n",
    "from brainlit.BrainLine.util import (\n",
    "    json_to_points,\n",
    "    find_atlas_level_label,\n",
    "    fold,\n",
    "    setup_atlas_graph,\n",
    "    get_atlas_level_nodes,\n",
    "    download_subvolumes,\n",
    ")\n",
    "from brainlit.BrainLine.apply_ilastik import ApplyIlastik, ApplyIlastik_LargeImage, plot_results, examine_threshold\n",
    "from brainlit.BrainLine.parse_ara import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from cloudreg.scripts.transform_points import NGLink\n",
    "from brainlit.BrainLine.imports import *\n",
    "\n",
    "%gui qt5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before Using this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Install brainlit, and other packages that this notebook uses\n",
    "### 1b. Write images to s3 using CloudReg\n",
    "    - e.g. python -m cloudreg.scripts.create_precomputed_volumes --s3_input_paths /mnt/NAS/SmartSPIM_Data/2022_03_02/20220302_14_40_04_8529_destriped_DONE/Ex_561_Em_600_stitched --s3_output_paths  s3://smartspim-precomputed-volumes/2022_03_02/8529/Ch_561_v2  --voxel_size 1.83 1.83 2 --num_procs 24 --resample_iso False\n",
    "### 1c. Make point annotations in neuroglancer to identify subvolumes for validation (and possible training)\n",
    "    - instructions: https://neurodata.io/help/neuroglancer-pt-annotations/\n",
    "    ,\n",
    "    {\n",
    "    \"type\":\"pointAnnotation\",\n",
    "    \"name\": \"soma_val\",\n",
    "    \"points\": []\n",
    "    },\n",
    "    {\n",
    "    \"type\":\"pointAnnotation\",\n",
    "    \"name\": \"nonsoma_val\",\n",
    "    \"points\":[]\n",
    "    }\n",
    "### 1d. Update soma_data.py file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainlit_path = Path(os.path.abspath(\"\"))\n",
    "brainlit_path = brainlit_path.parents[3]\n",
    "print(f\"Path to brainlit: {brainlit_path}\")\n",
    "\n",
    "for id in brain2paths.keys():\n",
    "    if \"base\" in brain2paths[id].keys() and \"val_info\" in brain2paths[id].keys():\n",
    "        base = brain2paths[id][\"base\"]\n",
    "        if \"http\" in base:\n",
    "            print(f\"Sample {id}: http in basepath, which may cause write errors\")\n",
    "\n",
    "        try:\n",
    "            url = brain2paths[id][\"val_info\"][\"url\"]\n",
    "            layer = brain2paths[id][\"val_info\"][\"somas_layer\"]\n",
    "            pts = json_to_points(url)[layer]\n",
    "            layer = brain2paths[id][\"val_info\"][\"nonsomas_layer\"]\n",
    "            pts = json_to_points(url)[layer]\n",
    "        except:\n",
    "            print(f\"Sample {id}: Error finding validation annotations with val_info\")\n",
    "\n",
    "        if \"train_info\" in brain2paths[id].keys():\n",
    "            try:\n",
    "                url = brain2paths[id][\"train_info\"][\"url\"]\n",
    "                layer = brain2paths[id][\"train_info\"][\"somas_layer\"]\n",
    "                pts = json_to_points(url)[layer]\n",
    "                layer = brain2paths[id][\"train_info\"][\"nonsomas_layer\"]\n",
    "                pts = json_to_points(url)[layer]\n",
    "            except:\n",
    "                print(\n",
    "                    f\"Sample {id}: Error finding training annotations with train_info\"\n",
    "                )\n",
    "    else:\n",
    "        print(f\"Sample {id}: Does not conform to desired format\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download benchmark data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"910\"  # brain ID\n",
    "soma_data_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/\"  # path to directory where training/validation data should be stored\n",
    "dataset_to_save = \"train\"  # train or val\n",
    "\n",
    "antibody_layer = \"Ch_647\"\n",
    "background_layer = \"Ch_561\"\n",
    "endogenous_layer = \"Ch_488\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvol_base = brain2paths[brain][\"base\"]\n",
    "layer_names = [antibody_layer, background_layer, endogenous_layer]\n",
    "\n",
    "if brain not in brain2paths.keys():\n",
    "    raise ValueError(f\"brain {brain} not an entry in brain2paths in soma_data.py file\")\n",
    "\n",
    "if f\"{dataset_to_save}_info\" not in brain2paths[\n",
    "    brain\n",
    "].keys() or dataset_to_save not in [\"train\", \"val\"]:\n",
    "    raise ValueError(f\"{dataset_to_save}_info not in brain2paths[{brain}].keys()\")\n",
    "\n",
    "\n",
    "for layer in [antibody_layer, background_layer, endogenous_layer]:\n",
    "    try:\n",
    "        CloudVolume(cvol_base + layer)\n",
    "    except:\n",
    "        print(f\"Sample {id}: Layer {layer} not found in {cvol_base}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_subvolumes(\n",
    "    soma_data_dir,\n",
    "    brain_id=brain,\n",
    "    layer_names=layer_names,\n",
    "    dataset_to_save=dataset_to_save,\n",
    "    object_type=\"soma\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View downloaded data (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brainr2/val/891_4202_1717_pos.h5\"  # path to file for viewing\n",
    "scale = [1.8, 1.8, 2]  # voxel size in microns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname, \"r\") as f:\n",
    "    pred = f.get(\"image_3channel\")\n",
    "    image_fg = pred[0, :, :, :]\n",
    "    image_bg = pred[1, :, :, :]\n",
    "    image_endo = pred[2, :, :, :]\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(image_fg, scale=scale)\n",
    "viewer.add_image(image_bg, scale=scale)\n",
    "viewer.add_image(image_endo, scale=scale)\n",
    "viewer.scale_bar.visible = True\n",
    "viewer.scale_bar.unit = \"um\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply ilastik to validation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this programmatically (below), or you can use the ilastik GUI (which is sometimes faster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"_rabies_pix_3ch\"\n",
    "project_path = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/matt_soma{model}.ilp\"  # path to ilastik model to be used\n",
    "ilastik_path = (\n",
    "    \"/Applications/ilastik-1.4.0b21-OSX.app/Contents/ilastik-release/run_ilastik.sh\"\n",
    ")\n",
    "brains_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/\"\n",
    "brains = [brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applyilastik = ApplyIlastik(\n",
    "    ilastk_path=ilastik_path,\n",
    "    project_path=project_path,\n",
    "    brains_path=brains_path,\n",
    "    brains=brains,\n",
    ")\n",
    "applyilastik.process_subvols()\n",
    "#applyilastik.move_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs (if relevant)\\*\n",
    "- identify files that have two somas in variable below. Since voxel coordinates are likely to be unique across samples, the file names below do not include sample IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles = [\n",
    "    \"3972_1636_1575_pos_Probabilities.h5\",\n",
    "    \"2867_4336_1296_pos_Probabilities.h5\",\n",
    "    \"2607_1845_1309_pos_Probabilities.h5\",\n",
    "    \"2101_3397_1747_pos_Probabilities.h5\",\n",
    "    \"2011_3452_1911_pos_Probabilities.h5\",\n",
    "    \"2113_3353_1727_pos_Probabilities.h5\",\n",
    "    \"1968_3472_1784_pos_Probabilities.h5\",\n",
    "]  # 8446"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(\n",
    "    data_dir=soma_data_dir,\n",
    "    brain_ids=[brain],\n",
    "    object_type=\"soma\",\n",
    "    positive_channel=0,\n",
    "    doubles=doubles,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_threshold(\n",
    "    data_dir=soma_data_dir,\n",
    "    brain_id=brain,\n",
    "    threshold=0.64,\n",
    "    object_type=\"soma\",\n",
    "    positive_channel=0,\n",
    "    doubles=doubles,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figure for all validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_ids = [\n",
    "    \"8607\",\n",
    "    \"8606\",\n",
    "    \"8477\",\n",
    "    \"8531\",\n",
    "    \"8608\",\n",
    "    \"8529\",\n",
    "    \"r1\",\n",
    "    \"r2\",\n",
    "    \"8446\",\n",
    "    \"8454\",\n",
    "    \"887\",\n",
    "    \"MPRRabies\",\n",
    "    \"969\",\n",
    "    \"910\"\n",
    "]\n",
    "\n",
    "plot_results(\n",
    "    data_dir=soma_data_dir,\n",
    "    brain_ids=brain_ids,\n",
    "    object_type=\"soma\",\n",
    "    positive_channel=0,\n",
    "    doubles=doubles,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Annotation layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_vol = CloudVolume(\n",
    "    \"precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017\"\n",
    ")\n",
    "for layer in [\n",
    "    antibody_layer,\n",
    "    background_layer,\n",
    "]:  # axon_mask is transformed into an image because nearest interpolation doesnt work well after downsampling\n",
    "    layer_path = brain2paths[brain][\"base\"] + layer + \"_transformed\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels=1,\n",
    "        layer_type=\"image\",\n",
    "        data_type=\"uint16\",  # Channel images might be 'uint8'\n",
    "        encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "        resolution=atlas_vol.resolution,  # Voxel scaling, units are in nanometers\n",
    "        voxel_offset=atlas_vol.voxel_offset,\n",
    "        chunk_size=[32, 32, 32],  # units are voxels\n",
    "        volume_size=atlas_vol.volume_size,  # e.g. a cubic millimeter dataset\n",
    "    )\n",
    "    vol_mask = CloudVolume(layer_path, info=info)\n",
    "    vol_mask.commit_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply ilastik to whole image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*\n",
    "You can use the notebook code below or the script using `soma_detect_image.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"test\"\n",
    "antibody_layer = \"antibody\"\n",
    "background_layer = \"background\"\n",
    "endogenous_layer = \"endogenous\"\n",
    "\n",
    "threshold = 0.28  # threshold to use for ilastik\n",
    "data_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainr_temp/\"  # \"/data/tathey1/matt_wright/brainr_temp/\"  # directory to store temporary subvolumes for segmentation\n",
    "results_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainr_results/\"  # directory to store coordinates of soma detections\n",
    "\n",
    "# Ilastik will run in \"headless mode\", and the following paths are needed to do so:\n",
    "ilastik_path = \"/Applications/ilastik-1.4.0b21-OSX.app/Contents/ilastik-release/run_ilastik.sh\"  # \"/data/tathey1/matt_wright/ilastik/ilastik-1.4.0rc5-Linux/run_ilastik.sh\"  # path to ilastik executable\n",
    "ilastik_project = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/matt_soma_rabies_pix_3ch.ilp\"  # \"/data/tathey1/matt_wright/ilastik/soma_model/matt_soma_rabies_pix_3ch.ilp\"  # path to ilastik project\n",
    "\n",
    "max_coords = [3072, 4352, 1792]  # -1 if you want to process the whole dimension\n",
    "ncpu = 1  # 16  # number of cores to use for detection\n",
    "chunk_size = [256, 256, 256]  # [256, 256, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [antibody_layer, background_layer, endogenous_layer]\n",
    "\n",
    "ilastik_largeimage = ApplyIlastik_LargeImage(\n",
    "    ilastik_path=ilastik_path,\n",
    "    ilastik_project=ilastik_project,\n",
    "    results_dir=results_dir,\n",
    "    ncpu=1,\n",
    ")\n",
    "ilastik_largeimage.apply_ilastik_parallel(\n",
    "    brain_id=brain,\n",
    "    layer_names=layer_names,\n",
    "    threshold=threshold,\n",
    "    data_dir=data_dir,\n",
    "    chunk_size=chunk_size,\n",
    "    max_coords=max_coords,\n",
    ")\n",
    "ilastik_largeimage.collect_results(brain_id=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Register volume and transform data to atlas space using CloudReg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. You need to find an initial affine alignment using cloudreg.scripts.registration.get_affine_matrix. For example: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A link to the ARA parcellation is:\n",
    "\n",
    "`precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017`\n",
    "\n",
    "And some python commands to help with affine alignment is:\n",
    "\n",
    "```\n",
    "from cloudreg.scripts.registration import get_affine_matrix\n",
    "get_affine_matrix([1,1,1], [0,0,0], \"PIR\", \"RPI\", 1.15, \"precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Run registration using cloudreg.scripts.registration. For example:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python -m cloudreg.scripts.registration -input_s3_path precomputed://s3://smartspim-precomputed-volumes/2023_03_15/969/Ch_561 --output_s3_path precomputed://s3://smartspim-precomputed-volumes/2023_03_15/969/atlas_to_target --atlas_s3_path https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_50um/average_50um --parcellation_s3_path https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017 --atlas_orientation PIR -orientation RPI --rotation 0 0 -5 --translation 0 0 0 --fixed_scale 1.05 -log_s3_path precomputed://s3://smartspim-precomputed-volumes/2023_03_15/969/atlas_to_target --missing_data_correction True --grid_correction False --bias_correction True --regularization 5000.0 --iterations 3000 --registration_resolution 100\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Transform data to atlas space using CloudReg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soma coordinates\n",
    "\n",
    "```\n",
    "python -m cloudreg.scripts.transform_points --target_viz_link \"https://viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=vMe2ijqQdk59Fg\" --atlas_viz_link \"https://ara.viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=HvyNDGaPsd1wyg\" --affine_path /mnt/NAS/Neuroglancer\\ Data/2022_03_15/8606_Ch_561_registration/downloop_1_A.mat  --velocity_path /mnt/NAS/Neuroglancer\\ Data/2022_03_15/8606_Ch_561_registration/downloop_1_v.mat --transformation_direction \"atlas\"\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "python -m cloudreg.scripts.transform_points --target_viz_link https://viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=05Fhxt5VBT-_1A --atlas_viz_link https://ara.viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=HvyNDGaPsd1wyg --affine_path /cis/home/tathey/MPRRabies_Ch_561_registration/downloop_1_A.mat   --velocity_path /cis/home/tathey/MPRRabies_Ch_561_registration/downloop_1_v.mat  --transformation_direction atlas\n",
    "```\n",
    "\n",
    "This will produce a neuroglancer link with the transformed soma coordinates, which should be added to `soma_data.py` under the `somas_atlas_url` key. Then the code below, or `soma_brainrender.py`, can be used to visualize the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image\n",
    "\n",
    "```\n",
    "python -m cloudreg.scripts.transform_data --target_layer_source precomputed://s3://smartspim-precomputed-volumes/2022_09_20/887/Ch_647 --transformed_layer_source precomputed://s3://smartspim-precomputed-volumes/2022_09_20/887/Ch_647_transformed --affine_path /cis/home/tathey/887_Ch_561_registration/downloop_1_A.mat  --velocity_path /cis/home/tathey/887_Ch_561_registration/downloop_1_v.mat\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View results in brain space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_ids = [\n",
    "    \"8557\",\n",
    "    \"8555\",\n",
    "    \"8607\",\n",
    "    \"8606\",\n",
    "    #\"8477\",\n",
    "    #\"8531\",\n",
    "    \"8608\",\n",
    "    #\"8529\",\n",
    "    #\"8454\",\n",
    "    \"MPRRabies\",\n",
    "]\n",
    "colors = {\n",
    "    \"tph2 vglut3\": \"blue\",\n",
    "    \"tph2 gad2\": \"red\",\n",
    "    # \"gad2 vgat\": \"green\",\n",
    "}  # colors for different genotypes\n",
    "symbols = [\"o\", \"+\", \"^\", \"vbar\"]\n",
    "fold_on = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SomaDistribution(brain_ids=brain_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/wholebrain_results/somas_formatt/\"\n",
    "\n",
    "brain = \"MPRRabies\"\n",
    "sd = SomaDistribution(brain_ids=[brain])\n",
    "\n",
    "for z in tqdm(np.arange(25,1320, 25)):\n",
    "    fname = dir + f\"{brain}_{z}.tif\"\n",
    "    v = sd.napari_coronal_section(\n",
    "        z=z, subtype_colors=colors, symbols=symbols, fold_on=fold_on\n",
    "    )\n",
    "    im = v.screenshot(canvas_only=False)\n",
    "    io.imsave(fname, im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.brainrender_somas(subtype_colors=colors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\n",
    "    688,  # cerebral cortex\n",
    "    95, # agranular insular area\n",
    "    714, # orbital area\n",
    "    698,  # olfactory areas\n",
    "    1089,  # hippocampal formation\n",
    "    # 583, # claustrum\n",
    "    477,  # striatum\n",
    "    # 803, # pallidum\n",
    "    351,  # bed nuclei of stria terminalis\n",
    "    # 703, #cortical subplate\n",
    "    1097,  # hypothalamus\n",
    "    157, #periventricular zone\n",
    "    515, # medial preoptic nucleus\n",
    "    290, # hypothalamic lateral zone\n",
    "    331, #mammillary body\n",
    "    797, # zona incerta\n",
    "    549,  # thalamus\n",
    "    186,  # lateral habenula\n",
    "    519,  # cerebellar nuclei\n",
    "    846, #dentate nucleus\n",
    "    726, # dentate gyrus\n",
    "    313,  # midbrain\n",
    "    157, #inferior colliculus\n",
    "    1052, #pedunculopontine\n",
    "    128, #midbrain reticular nucleus\n",
    "    214, # red nucleus\n",
    "    1065,  # hindbrain\n",
    "    867, #parabrachial nucleus\n",
    "    701, #vestibular nuclei\n",
    "    972, # prelimbic\n",
    "    44, # infralimbic\n",
    "]  # allen atlas region IDs to be shown\n",
    "# see: https://connectivity.brain-map.org/projection/experiment/480074702?imageId=480075280&initImage=TWO_PHOTON&x=17028&y=11704&z=3\n",
    "\n",
    "composite_regions = {\n",
    "    \"Amygdalar Nuclei\": [131, 295, 319, 780], \"Substantia Nigra\": [615, 374, 374], \"Superior Colliculus\": [294, 302]\n",
    "}  # Custom composite allen regions where key is region name and value is list of allen regions\n",
    "\n",
    "brain_ids = [\"8557\",\"8555\",\"8607\",\"8606\",\"8477\", \"8531\", \"8608\", \"8529\", \"8454\", \"8446\", \"MPRRabies\"]\n",
    "sd = SomaDistribution(brain_ids=brain_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_regioncounts = {}\n",
    "atlas_points = {}\n",
    "\n",
    "for brain_id in brain_ids:\n",
    "    pkl_file = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/wholebrain_results/quantification_dict_{brain_id}.pickle\"\n",
    "    with open(pkl_file, \"rb\") as handle:\n",
    "        q_dict = pickle.load(handle)\n",
    "    id_to_regioncounts[brain_id] = q_dict\n",
    "\n",
    "    total_somas = 0\n",
    "    for region in q_dict.keys():\n",
    "        total_somas += q_dict[region]\n",
    "    atlas_points[brain_id] = np.zeros((total_somas,0))\n",
    "\n",
    "sd.atlas_points = atlas_points\n",
    "sd.id_to_regioncounts = id_to_regioncounts\n",
    "sd.region_graph = sd._setup_regiongraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.region_barchart(regions, composite_regions=composite_regions, normalize_region=872)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare distributions with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "brain_ids = []\n",
    "genotypes = []\n",
    "\n",
    "for i, brain in enumerate(brains):\n",
    "    print(brain)\n",
    "    region_order = list(df.loc[df[\"Brain ID\"] == brain][\"Region\"])\n",
    "\n",
    "    if i == 0:\n",
    "        standard_region_order = region_order\n",
    "    elif standard_region_order != region_order:\n",
    "        raise ValueError(f\"Different region order for brain {brain}\")\n",
    "\n",
    "    distrib = list(df.loc[df[\"Brain ID\"] == brain][\"Percent of Total Somas (%)\"])\n",
    "    X.append(distrib)\n",
    "\n",
    "    brain_ids.append(brain)\n",
    "    genotypes.append(brains[brain])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2 = pca.fit_transform(X)\n",
    "\n",
    "df_pca = {\n",
    "    \"PC 1\": X_2[:, 0],\n",
    "    \"PC 2\": X_2[:, 1],\n",
    "    \"Genotype\": genotypes,\n",
    "    \"Brain ID\": brain_ids,\n",
    "}\n",
    "df_pca = pd.DataFrame(data=df_pca)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.scatterplot(data=df_pca, x=\"PC 1\", y=\"PC 2\", hue=\"Genotype\", ax=ax)\n",
    "\n",
    "\n",
    "for i in range(df_pca.shape[0]):\n",
    "    plt.text(\n",
    "        x=df_pca[\"PC 1\"][i] + 0.03,\n",
    "        y=df_pca[\"PC 2\"][i] + 0.03,\n",
    "        s=df_pca[\"Brain ID\"][i],\n",
    "        fontdict=dict(color=\"black\", size=20),\n",
    "    )\n",
    "\n",
    "plt.title(\n",
    "    f\"Input Distribution PCA with Explained Variance: {pca.explained_variance_ratio_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create local volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = CloudVolume(\n",
    "    \"precomputed://s3://smartspim-precomputed-volumes/2022_03_10/8531/Ch_647_iso\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvol = vol[256 * 11 : 256 * 12, 256 * 16 : 256 * 17, 256 * 6 : 256 * 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = CloudVolume.create_new_info(\n",
    "    num_channels=1,\n",
    "    layer_type=\"image\",\n",
    "    data_type=\"uint16\",  # Channel images might be 'uint8'\n",
    "    encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution=vol.resolution,  # Voxel scaling, units are in nanometers\n",
    "    voxel_offset=vol.voxel_offset,\n",
    "    chunk_size=vol.chunk_size,  # units are voxels\n",
    "    volume_size=vol.volume_size,  # e.g. a cubic millimeter dataset\n",
    ")\n",
    "\n",
    "vol_ex = CloudVolume(\n",
    "    \"precomputed://file:///Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/brainlit/BrainLine/data/example\",\n",
    "    info=info,\n",
    "    compress=False,\n",
    ")\n",
    "vol_ex.commit_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_ex[256 * 11 : 256 * 12, 256 * 16 : 256 * 17, 256 * 6 : 256 * 7] = subvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = CloudVolume(\n",
    "    \"precomputed://file:///Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/brainlit/BrainLine/data/example/endogenous\",\n",
    "    fill_missing=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post detections to ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brain_id = \"8606\"\n",
    "all_somas_pth = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/wholebrain_results/all_somas_{brain_id}.txt\"\n",
    "\n",
    "from brainlit.BrainLine.data.soma_data import brain2paths\n",
    "from cloudreg.scripts.transform_points import NGLink\n",
    "from cloudreg.scripts.visualization import create_viz_link_from_json\n",
    "\n",
    "coords = []\n",
    "coords_target_space = []\n",
    "file1 = open(all_somas_pth, \"r\")\n",
    "lines = file1.readlines()\n",
    "for line in tqdm(lines, desc=\"parsing coordinates\", leave=False):\n",
    "    if line != \"\\n\":\n",
    "        line = \" \".join(line.split())\n",
    "        elements = line.split(\",\")\n",
    "        coord = [elements[0][1:], elements[1], elements[2][:-1]]\n",
    "\n",
    "        coords.append([float(e.strip()) for e in coord])\n",
    "\n",
    "coords = coords[15000:]\n",
    "print(f\"{len(coords)} detections\")\n",
    "\n",
    "ng_link = brain2paths[brain_id][\"val_info\"][\"url\"]\n",
    "viz_link = NGLink(ng_link.split(\"json_url=\")[-1])\n",
    "ngl_json = viz_link._json\n",
    "\n",
    "ngl_json[\"layers\"] = [\n",
    "    layer for layer in ngl_json[\"layers\"] if layer[\"type\"] != \"annotation\"\n",
    "]\n",
    "\n",
    "ngl_json[\"layers\"].append(\n",
    "    {\"type\": \"annotation\", \"points\": coords, \"name\": \"detected_somas\"}\n",
    ")\n",
    "viz_link = create_viz_link_from_json(\n",
    "    ngl_json, neuroglancer_link=\"https://viz.neurodata.io/?json_url=\"\n",
    ")\n",
    "print(f\"Viz link with detections: {viz_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('docs_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
