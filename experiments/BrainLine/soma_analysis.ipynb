{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soma Detection Analysis of Whole-Brain Light-Sheet Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Before Using this notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Install brainlit, and other packages that this notebook uses\n",
    "### 1b. Write images to s3 using CloudReg\n",
    "    - e.g. python -m cloudreg.scripts.create_precomputed_volumes --s3_input_paths /mnt/NAS/SmartSPIM_Data/2022_03_02/20220302_14_40_04_8529_destriped_DONE/Ex_561_Em_600_stitched --s3_output_paths  s3://smartspim-precomputed-volumes/2022_03_02/8529/Ch_561_v2  --voxel_size 1.83 1.83 2 --num_procs 24 --resample_iso False\n",
    "### 1c. Make point annotations in neuroglancer to identify subvolumes for validation (and possible training)\n",
    "    - instructions: https://neurodata.io/help/neuroglancer-pt-annotations/\n",
    "    ,\n",
    "    {\n",
    "    \"type\":\"pointAnnotation\",\n",
    "    \"name\": \"soma_val\",\n",
    "    \"points\": []\n",
    "    },\n",
    "    {\n",
    "    \"type\":\"pointAnnotation\",\n",
    "    \"name\": \"nonsoma_val\",\n",
    "    \"points\":[]\n",
    "    }\n",
    "### 1d. Update soma_data.py file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainlit.preprocessing import removeSmallCCs\n",
    "from brainlit.BrainLine.data.soma_data import brain2paths, brain2centers\n",
    "from brainlit.BrainLine.analyze_results import SomaDistribution\n",
    "from brainlit.BrainLine.util import (\n",
    "    json_to_points,\n",
    "    find_atlas_level_label,\n",
    "    fold,\n",
    "    setup_atlas_graph,\n",
    "    get_atlas_level_nodes,\n",
    "    download_subvolumes,\n",
    ")\n",
    "from brainlit.BrainLine.apply_ilastik import ApplyIlastik, ApplyIlastik_LargeImage\n",
    "from brainlit.BrainLine.parse_ara import *\n",
    "import xml.etree.ElementTree as ET\n",
    "from cloudreg.scripts.transform_points import NGLink\n",
    "from brainlit.BrainLine.imports import *\n",
    "\n",
    "%gui qt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brainlit_path = Path(os.path.abspath(\"\"))\n",
    "brainlit_path = brainlit_path.parents[3]\n",
    "print(f\"Path to brainlit: {brainlit_path}\")\n",
    "\n",
    "for id in brain2paths.keys():\n",
    "    if \"base\" in brain2paths[id].keys() and \"val_info\" in brain2paths[id].keys():\n",
    "        base = brain2paths[id][\"base\"]\n",
    "        if \"http\" in base:\n",
    "            print(f\"Sample {id}: http in basepath, which may cause write errors\")\n",
    "\n",
    "        try:\n",
    "            url = brain2paths[id][\"val_info\"][\"url\"]\n",
    "            layer = brain2paths[id][\"val_info\"][\"somas_layer\"]\n",
    "            pts = json_to_points(url)[layer]\n",
    "            layer = brain2paths[id][\"val_info\"][\"nonsomas_layer\"]\n",
    "            pts = json_to_points(url)[layer]\n",
    "        except:\n",
    "            print(f\"Sample {id}: Error finding validation annotations with val_info\")\n",
    "\n",
    "        if \"train_info\" in brain2paths[id].keys():\n",
    "            try:\n",
    "                url = brain2paths[id][\"train_info\"][\"url\"]\n",
    "                layer = brain2paths[id][\"train_info\"][\"somas_layer\"]\n",
    "                pts = json_to_points(url)[layer]\n",
    "                layer = brain2paths[id][\"train_info\"][\"nonsomas_layer\"]\n",
    "                pts = json_to_points(url)[layer]\n",
    "            except:\n",
    "                print(\n",
    "                    f\"Sample {id}: Error finding training annotations with train_info\"\n",
    "                )\n",
    "    else:\n",
    "        print(f\"Sample {id}: Does not conform to desired format\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download benchmark data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"887\"  # brain ID\n",
    "soma_data_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/\"  # path to directory where training/validation data should be stored\n",
    "dataset_to_save = \"val\"  # train or val\n",
    "\n",
    "antibody_layer = \"Ch_647\"\n",
    "background_layer = \"Ch_561\"\n",
    "endogenous_layer = \"Ch_488\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [antibody_layer, background_layer, endogenous_layer]\n",
    "\n",
    "if brain not in brain2paths.keys():\n",
    "    raise ValueError(f\"brain {brain} not an entry in brain2paths in axon_data.py file\")\n",
    "\n",
    "if f\"{dataset_to_save}_info\" not in brain2paths[\n",
    "    brain\n",
    "].keys() or dataset_to_save not in [\"train\", \"val\"]:\n",
    "    raise ValueError(f\"{dataset_to_save}_info not in brain2paths[{brain}].keys()\")\n",
    "\n",
    "\n",
    "for layer in [antibody_layer, background_layer, endogenous_layer]:\n",
    "    try:\n",
    "        CloudVolume(base + layer)\n",
    "    except:\n",
    "        print(f\"Sample {id}: Layer {layer} not found in {base}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_subvolumes(\n",
    "    soma_data_dir,\n",
    "    brain_id=brain,\n",
    "    layer_names=layer_names,\n",
    "    dataset_to_save=dataset_to_save,\n",
    "    object_type=\"soma\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View downloaded data (optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brainr2/val/891_4202_1717_pos.h5\"  # path to file for viewing\n",
    "scale = [1.8, 1.8, 2]  # voxel size in microns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(fname, \"r\") as f:\n",
    "    pred = f.get(\"image_3channel\")\n",
    "    image_fg = pred[0, :, :, :]\n",
    "    image_bg = pred[1, :, :, :]\n",
    "    image_endo = pred[2, :, :, :]\n",
    "\n",
    "viewer = napari.Viewer(ndisplay=3)\n",
    "viewer.add_image(image_fg, scale=scale)\n",
    "viewer.add_image(image_bg, scale=scale)\n",
    "viewer.add_image(image_endo, scale=scale)\n",
    "viewer.scale_bar.visible = True\n",
    "viewer.scale_bar.unit = \"um\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply ilastik to validation data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do this programmatically (below), or you can use the ilastik GUI (which is sometimes faster)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"_rabies_pix_3ch\"\n",
    "project_path = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/matt_soma{model}.ilp\"  # path to ilastik model to be used\n",
    "ilastik_path = (\n",
    "    \"/Applications/ilastik-1.4.0b21-OSX.app/Contents/ilastik-release/run_ilastik.sh\"\n",
    ")\n",
    "brains_path = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/\"\n",
    "brains_path = \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/\"\n",
    "brains = [\"test\"]  # [brain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applyilastik = ApplyIlastik(\n",
    "    ilastk_path=ilastik_path,\n",
    "    project_path=project_path,\n",
    "    brains_path=brains_path,\n",
    "    brains=brains,\n",
    ")\n",
    "applyilastik.process_somas()\n",
    "applyilastik.move_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs (if relevant)\\*\n",
    "- identify files that have two somas in variable below. Since voxel coordinates are likely to be unique across samples, the file names below do not include sample IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles = [\n",
    "    \"3972_1636_1575_pos_Probabilities.h5\",\n",
    "    \"2867_4336_1296_pos_Probabilities.h5\",\n",
    "    \"2607_1845_1309_pos_Probabilities.h5\",\n",
    "    \"2101_3397_1747_pos_Probabilities.h5\",\n",
    "    \"2011_3452_1911_pos_Probabilities.h5\",\n",
    "    \"2113_3353_1727_pos_Probabilities.h5\",\n",
    "    \"1968_3472_?1784_pos_Probabilities.h5\",\n",
    "]  # 8446"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "files_dir = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain}/val/\"\n",
    "onlyfiles = [\n",
    "    f for f in os.listdir(files_dir) if os.path.isfile(os.path.join(files_dir, f))\n",
    "]\n",
    "test_files = [f for f in onlyfiles if \"Probabilities\" in f]  # \"probabilities\"\n",
    "print(test_files)\n",
    "\n",
    "size_thresh = 500\n",
    "\n",
    "thresholds = list(np.arange(0.0, 1.0, 0.02))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    tot_pos = 0\n",
    "    tot_neg = 0\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    for filename in tqdm(test_files, disable=True):\n",
    "        if filename in doubles:\n",
    "            newpos = 2\n",
    "        else:\n",
    "            newpos = 1\n",
    "\n",
    "        fname = files_dir + filename\n",
    "        f = h5py.File(fname, \"r\")\n",
    "        pred = f.get(\"exported_data\")\n",
    "        pred = pred[0, :, :, :]\n",
    "        mask = pred > threshold\n",
    "        labels = measure.label(mask)\n",
    "        props = measure.regionprops(labels)\n",
    "\n",
    "        if \"pos\" in filename:\n",
    "            num_detected = 0\n",
    "            tot_pos += newpos\n",
    "            for prop in props:\n",
    "                if prop[\"area\"] > size_thresh:\n",
    "                    if num_detected < newpos:\n",
    "                        true_pos += 1\n",
    "                        num_detected += 1\n",
    "                    else:\n",
    "                        false_pos += 1\n",
    "        elif \"neg\" in filename:\n",
    "            tot_neg += 1\n",
    "            for prop in props:\n",
    "                if prop[\"area\"] > size_thresh:\n",
    "                    false_pos += 1\n",
    "\n",
    "    recall = true_pos / tot_pos\n",
    "    recalls.append(recall)\n",
    "    if true_pos + false_pos == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "    precisions.append(precision)\n",
    "    if precision == 0 and recall == 0:\n",
    "        fscore = 0\n",
    "    else:\n",
    "        fscore = 2 * precision * recall / (precision + recall)\n",
    "    print(\n",
    "        f\"threshold: {threshold}: precision: {precision}, recall: {recall}, f-score: {fscore} for {tot_pos} positive samples in {len(test_files)} images\"\n",
    "    )\n",
    "\n",
    "fscores = [\n",
    "    2 * precision * recall / (precision + recall)\n",
    "    if (precision != 0 and recall != 0)\n",
    "    else 0\n",
    "    for precision, recall in zip(precisions, recalls)\n",
    "]\n",
    "dict = {\n",
    "    \"Recall\": recalls,\n",
    "    \"Precision\": precisions,\n",
    "    \"F-score\": fscores,\n",
    "    \"Threshold\": thresholds,\n",
    "}\n",
    "df = pd.DataFrame(dict)\n",
    "max_fscore = df[\"F-score\"].max()\n",
    "best_threshold = float(df.loc[df[\"F-score\"] == max_fscore][\"Threshold\"].iloc[0])\n",
    "best_rec = float(df.loc[df[\"F-score\"] == max_fscore][\"Recall\"].iloc[0])\n",
    "best_prec = float(df.loc[df[\"F-score\"] == max_fscore][\"Precision\"].iloc[0])\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.lineplot(data=df, x=\"Recall\", y=\"Precision\", estimator=np.amax, ci=False)\n",
    "plt.scatter(\n",
    "    best_rec,\n",
    "    best_prec,\n",
    "    c=\"r\",\n",
    "    label=f\"Max f-score: {max_fscore:.2f} thresh:{best_threshold:.2f}\",\n",
    ")\n",
    "plt.xlim([0, 1.1])\n",
    "plt.ylim([0, 1.1])\n",
    "plt.title(f\"Brain {brain} Validation: {tot_pos}+ {tot_neg}-\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If results above are not adequate, improve model and try again\n",
    "\n",
    "In my case, I identify more subvolumes from the sample at hand using the same process as for validation data, and add it as training data to the model and retrain."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(test_files, disable=True):\n",
    "    print(f\"*************File: {filename}*********\")\n",
    "    if filename in doubles:\n",
    "        newpos = 2\n",
    "    else:\n",
    "        newpos = 1\n",
    "\n",
    "    im_fname = files_dir + filename[:-17] + \".h5\"\n",
    "    fname = files_dir + filename\n",
    "    f = h5py.File(fname, \"r\")\n",
    "    pred = f.get(\"exported_data\")\n",
    "    pred = pred[0, :, :, :]\n",
    "    mask = pred > best_threshold\n",
    "    labels = measure.label(mask)\n",
    "    props = measure.regionprops(labels)\n",
    "\n",
    "    if \"pos\" in filename:\n",
    "        num_detected = 0\n",
    "        tot_pos += newpos\n",
    "        for prop in props:\n",
    "            area = prop[\"area\"]\n",
    "            if area > size_thresh:\n",
    "                print(f\"area of detected object: {area}\")\n",
    "                if num_detected < newpos:\n",
    "                    true_pos += 1\n",
    "                    num_detected += 1\n",
    "                else:\n",
    "                    print(f\"Soma false positive Area: {area}\")\n",
    "                    f = h5py.File(im_fname, \"r\")\n",
    "                    im = f.get(\"image_3channel\")\n",
    "                    viewer = napari.Viewer(ndisplay=3)\n",
    "                    viewer.add_image(im[0, :, :, :], name=filename)\n",
    "                    viewer.add_image(im[1, :, :, :], name=\"bg\")\n",
    "                    viewer.add_image(im[2, :, :, :], name=\"endo\")\n",
    "                    viewer.add_labels(mask)\n",
    "                    viewer.add_labels(\n",
    "                        labels == prop[\"label\"],\n",
    "                        name=f\"soma false positive area: {area}\",\n",
    "                    )\n",
    "                    false_pos += 1\n",
    "        if num_detected == 0:\n",
    "            print(f\"Soma false negative\")\n",
    "            f = h5py.File(im_fname, \"r\")\n",
    "            im = f.get(\"image_3channel\")\n",
    "            viewer = napari.Viewer(ndisplay=3)\n",
    "            viewer.add_image(im[0, :, :, :], name=filename)\n",
    "            viewer.add_image(im[1, :, :, :], name=\"bg\")\n",
    "            viewer.add_image(im[2, :, :, :], name=\"endo\")\n",
    "            viewer.add_labels(mask, name=\"Soma false negative\")\n",
    "    elif \"neg\" in filename:\n",
    "        for prop in props:\n",
    "            area = prop[\"area\"]\n",
    "            if area > size_thresh:\n",
    "                print(f\"Nonsoma false positive Area: {area}\")\n",
    "                f = h5py.File(im_fname, \"r\")\n",
    "                im = f.get(\"image_3channel\")\n",
    "                viewer = napari.Viewer(ndisplay=3)\n",
    "                viewer.add_image(im[0, :, :, :], name=filename)\n",
    "                viewer.add_image(im[1, :, :, :], name=\"bg\")\n",
    "                viewer.add_image(im[2, :, :, :], name=\"endo\")\n",
    "                viewer.add_labels(mask)\n",
    "                viewer.add_labels(\n",
    "                    labels == prop[\"label\"], name=f\"nonsoma false positive area: {area}\"\n",
    "                )\n",
    "                false_pos += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper figure for all validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brains = [\n",
    "    \"8607\",\n",
    "    \"8606\",\n",
    "    \"8477\",\n",
    "    \"8531\",\n",
    "    \"8608\",\n",
    "    \"8529\",\n",
    "    \"8557\",\n",
    "    \"8555\",\n",
    "    \"8446\",\n",
    "    \"8454\",\n",
    "    \"887\",\n",
    "]\n",
    "\n",
    "recalls = []\n",
    "precisions = []\n",
    "brain_ids = []\n",
    "\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_fscores = {}\n",
    "\n",
    "for brain_id in tqdm(brains, desc=\"Computing validation...\"):\n",
    "    if brain_id == \"8557\":\n",
    "        brain_name = \"r1\"\n",
    "    elif brain_id == \"8555\":\n",
    "        brain_name = \"r2\"\n",
    "    else:\n",
    "        brain_name = brain_id\n",
    "\n",
    "    files_dir = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain_name}/val/\"\n",
    "    onlyfiles = [\n",
    "        f for f in os.listdir(files_dir) if os.path.isfile(os.path.join(files_dir, f))\n",
    "    ]\n",
    "    test_files = [f for f in onlyfiles if \"Probabilities\" in f]\n",
    "\n",
    "    best_fscore = 0\n",
    "\n",
    "    size_thresh = 500\n",
    "\n",
    "    thresholds = list(np.arange(0.0, 1.0, 0.02))\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        tot_pos = 0\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        for filename in tqdm(test_files, disable=True):\n",
    "            if filename in doubles:\n",
    "                newpos = 2\n",
    "            else:\n",
    "                newpos = 1\n",
    "\n",
    "            fname = files_dir + filename\n",
    "            f = h5py.File(fname, \"r\")\n",
    "            pred = f.get(\"exported_data\")\n",
    "            pred = pred[0, :, :, :]\n",
    "            mask = pred > threshold\n",
    "            labels = measure.label(mask)\n",
    "            props = measure.regionprops(labels)\n",
    "\n",
    "            if \"pos\" in filename:\n",
    "                num_detected = 0\n",
    "                tot_pos += newpos\n",
    "                for prop in props:\n",
    "                    if prop[\"area\"] > size_thresh:\n",
    "                        if num_detected < newpos:\n",
    "                            true_pos += 1\n",
    "                            num_detected += 1\n",
    "                        else:\n",
    "                            false_pos += 1\n",
    "            elif \"neg\" in filename:\n",
    "                for prop in props:\n",
    "                    if prop[\"area\"] > size_thresh:\n",
    "                        false_pos += 1\n",
    "\n",
    "        recall = true_pos / tot_pos\n",
    "        recalls.append(recall)\n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 1\n",
    "        else:\n",
    "            precision = true_pos / (true_pos + false_pos)\n",
    "\n",
    "        precisions.append(precision)\n",
    "        if precision == 0 and recall == 0:\n",
    "            fscore = 0\n",
    "        else:\n",
    "            fscore = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "        if fscore > best_fscore:\n",
    "            best_fscore = fscore\n",
    "            best_prec = precision\n",
    "            best_recall = recall\n",
    "\n",
    "        brain_ids.append(brain_id)\n",
    "    best_fscores[brain_id] = best_fscore\n",
    "    best_precisions.append(best_prec)\n",
    "    best_recalls.append(best_recall)\n",
    "\n",
    "\n",
    "for i, brain_id in enumerate(brain_ids):\n",
    "    brain_ids[i] = brain_id + f\" - Max F-score: {best_fscores[brain_id]:.2f}\"\n",
    "\n",
    "data = {\"Sample\": brain_ids, \"Recall\": recalls, \"Precision\": precisions}\n",
    "df = pd.DataFrame(data=data)\n",
    "\n",
    "sns.set(rc={\"figure.figsize\": (10, 7)})\n",
    "sns.set(font_scale=2)\n",
    "sns.lineplot(data=df, x=\"Recall\", y=\"Precision\", hue=\"Sample\", estimator=\"max\", ci=None)\n",
    "sns.scatterplot(x=best_recalls, y=best_precisions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make Annotation layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformed layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_vol = CloudVolume(\n",
    "    \"precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017\"\n",
    ")\n",
    "for layer in [\n",
    "    antibody_layer,\n",
    "    background_layer,\n",
    "]:  # axon_mask is transformed into an image because nearest interpolation doesnt work well after downsampling\n",
    "    layer_path = brain2paths[brain][\"base\"] + layer + \"_transformed\"\n",
    "    info = CloudVolume.create_new_info(\n",
    "        num_channels=1,\n",
    "        layer_type=\"image\",\n",
    "        data_type=\"uint16\",  # Channel images might be 'uint8'\n",
    "        encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "        resolution=atlas_vol.resolution,  # Voxel scaling, units are in nanometers\n",
    "        voxel_offset=atlas_vol.voxel_offset,\n",
    "        chunk_size=[32, 32, 32],  # units are voxels\n",
    "        volume_size=atlas_vol.volume_size,  # e.g. a cubic millimeter dataset\n",
    "    )\n",
    "    vol_mask = CloudVolume(layer_path, info=info)\n",
    "    vol_mask.commit_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Apply ilastik to whole image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\* Inputs \\*\n",
    "You can use the notebook code below or the script using `soma_detect_image.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain = \"test\"\n",
    "antibody_layer = \"antibody\"\n",
    "background_layer = \"background\"\n",
    "endogenous_layer = \"endogenous\"\n",
    "\n",
    "threshold = 0.28  # threshold to use for ilastik\n",
    "data_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainr_temp/\"  # \"/data/tathey1/matt_wright/brainr_temp/\"  # directory to store temporary subvolumes for segmentation\n",
    "results_dir = \"/Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainr_results/\"  # directory to store coordinates of soma detections\n",
    "\n",
    "# Ilastik will run in \"headless mode\", and the following paths are needed to do so:\n",
    "ilastik_path = \"/Applications/ilastik-1.4.0b21-OSX.app/Contents/ilastik-release/run_ilastik.sh\"  # \"/data/tathey1/matt_wright/ilastik/ilastik-1.4.0rc5-Linux/run_ilastik.sh\"  # path to ilastik executable\n",
    "ilastik_project = \"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/matt_soma_rabies_pix_3ch.ilp\"  # \"/data/tathey1/matt_wright/ilastik/soma_model/matt_soma_rabies_pix_3ch.ilp\"  # path to ilastik project\n",
    "\n",
    "max_coords = [3072, 4352, 1792]  # -1 if you want to process the whole dimension\n",
    "ncpu = 1  # 16  # number of cores to use for detection\n",
    "chunk_size = [256, 256, 256]  # [256, 256, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = [antibody_layer, background_layer, endogenous_layer]\n",
    "\n",
    "ilastik_largeimage = ApplyIlastik_LargeImage(\n",
    "    ilastik_path=ilastik_path,\n",
    "    ilastik_project=ilastik_project,\n",
    "    results_dir=results_dir,\n",
    "    ncpu=1,\n",
    ")\n",
    "ilastik_largeimage.apply_ilastik_parallel(\n",
    "    brain_id=brain,\n",
    "    layer_names=layer_names,\n",
    "    threshold=threshold,\n",
    "    data_dir=data_dir,\n",
    "    chunk_size=chunk_size,\n",
    "    max_coords=max_coords,\n",
    ")\n",
    "ilastik_largeimage.collect_results(brain_id=\"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Register volume and transform data to atlas space using CloudReg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. You need to find an initial affine alignment using cloudreg.scripts.registration.get_affine_matrix. For example: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A link to the ARA parcellation is:\n",
    "\n",
    "`precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017`\n",
    "\n",
    "And some python commands to help with affine alignment is:\n",
    "\n",
    "```\n",
    "from cloudreg.scripts.registration import get_affine_matrix\n",
    "get_affine_matrix([1,1,1], [15,0,0], \"PIR\", \"RAI\", 1.15, \"precomputed://https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Run registration using cloudreg.scripts.registration. For example:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python -m cloudreg.scripts.registration -input_s3_path precomputed://s3://smartspim-precomputed-volumes/2023_01_20/MPRRabies/Ch_561 --output_s3_path precomputed://s3://smartspim-precomputed-volumes/2023_01_20/MPRRabies/atlas_to_target --atlas_s3_path https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_50um/average_50um --parcellation_s3_path https://open-neurodata.s3.amazonaws.com/ara_2016/sagittal_10um/annotation_10um_2017 --atlas_orientation PIR -orientation RPI --rotation 0 0 0 --translation 0 0 0 --fixed_scale 1.07 -log_s3_path precomputed://s3://smartspim-precomputed-volumes/2023_01_20/MPRRabies/atlas_to_target --missing_data_correction True --grid_correction False --bias_correction True --regularization 5000.0 --iterations 3000 --registration_resolution 100\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Transform data to atlas space using CloudReg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soma coordinates\n",
    "\n",
    "```\n",
    "python -m cloudreg.scripts.transform_points --target_viz_link https://viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=6ti276yAxXF_Rw --atlas_viz_link https://ara.viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=HvyNDGaPsd1wyg --affine_path /mnt/NAS/Neuroglancer\\ Data/  --velocity_path /mnt/NAS/Neuroglancer\\ Data/  --transformation_direction atlas\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "python -m cloudreg.scripts.transform_points --target_viz_link https://viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=05Fhxt5VBT-_1A --atlas_viz_link https://ara.viz.neurodata.io/?json_url=https://json.neurodata.io/v1?NGStateID=HvyNDGaPsd1wyg --affine_path /cis/home/tathey/MPRRabies_Ch_561_registration/downloop_1_A.mat   --velocity_path /cis/home/tathey/MPRRabies_Ch_561_registration/downloop_1_v.mat  --transformation_direction atlas\n",
    "```\n",
    "\n",
    "This will produce a neuroglancer link with the transformed soma coordinates, which should be added to `soma_data.py` under the `somas_atlas_url` key. Then the code below, or `soma_brainrender.py`, can be used to visualize the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image\n",
    "\n",
    "```\n",
    "python -m cloudreg.scripts.transform_data --target_layer_source precomputed://s3://smartspim-precomputed-volumes/2022_09_20/887/Ch_647 --transformed_layer_source precomputed://s3://smartspim-precomputed-volumes/2022_09_20/887/Ch_647_transformed --affine_path /cis/home/tathey/887_Ch_561_registration/downloop_1_A.mat  --velocity_path /cis/home/tathey/887_Ch_561_registration/downloop_1_v.mat\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View results in brain space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\*Inputs\\*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_ids = [\n",
    "    \"8557\",\n",
    "    \"8555\",\n",
    "    \"8607\",\n",
    "    \"8606\",\n",
    "    \"8477\",\n",
    "    \"8531\",\n",
    "    \"8608\",\n",
    "    \"8529\",\n",
    "    \"8454\",\n",
    "    \"MPRRabies\",\n",
    "]\n",
    "colors = {\n",
    "    \"tph2 vglut3\": \"blue\",\n",
    "    \"tph2 gad2\": \"red\",\n",
    "    \"gad2 vgat\": \"green\",\n",
    "}  # colors for different genotypes\n",
    "symbols = [\"o\", \"+\", \"^\", \"vbar\"]\n",
    "fold_on = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SomaDistribution(brain_ids=brain_ids)\n",
    "sd.napari_coronal_section(\n",
    "    z=1000, subtype_colors=colors, symbols=symbols, fold_on=fold_on\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SomaDistribution(brain_ids=brain_ids)\n",
    "sd.brainrender_somas(subtype_colors=colors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\n",
    "    688,  # cerebral cortex\n",
    "    95, # agranular insular area\n",
    "    714, # orbital area\n",
    "    698,  # olfactory areas\n",
    "    1089,  # hippocampal formation\n",
    "    # 583, # claustrum\n",
    "    477,  # striatum\n",
    "    # 803, # pallidum\n",
    "    351,  # bed nuclei of stria terminalis\n",
    "    # 703, #cortical subplate\n",
    "    1097,  # hypothalamus\n",
    "    157, #periventricular zone\n",
    "    515, # medial preoptic nucleus\n",
    "    290, # hypothalamic lateral zone\n",
    "    331, #mammillary body\n",
    "    797, # zona incerta\n",
    "    549,  # thalamus\n",
    "    186,  # lateral habenula\n",
    "    519,  # cerebellar nuclei\n",
    "    846, #dentate nucleus\n",
    "    726, # dentate gyrus\n",
    "    313,  # midbrain\n",
    "    157, #inferior colliculus\n",
    "    1052, #pedunculopontine\n",
    "    128, #midbrain reticular nucleus\n",
    "    214, # red nucleus\n",
    "    1065,  # hindbrain\n",
    "    867, #parabrachial nucleus\n",
    "    701, #vestibular nuclei\n",
    "    972, # prelimbic\n",
    "    44, # infralimbic\n",
    "]  # allen atlas region IDs to be shown\n",
    "# see: https://connectivity.brain-map.org/projection/experiment/480074702?imageId=480075280&initImage=TWO_PHOTON&x=17028&y=11704&z=3\n",
    "\n",
    "composite_regions = {\n",
    "    \"Amygdalar Nuclei\": [131, 295, 319, 780], \"Substantia Nigra\": [615, 374, 374], \"Superior Colliculus\": [294, 302]\n",
    "}  # Custom composite allen regions where key is region name and value is list of allen regions\n",
    "\n",
    "brain_ids = [\"8557\",\"8555\",\"8607\",\"8606\",\"8477\", \"8531\", \"8608\", \"8529\", \"8454\", \"8446\", \"MPRRabies\"]\n",
    "sd = SomaDistribution(brain_ids=brain_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_regioncounts = {}\n",
    "atlas_points = {}\n",
    "\n",
    "for brain_id in brain_ids:\n",
    "    pkl_file = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/wholebrain_results/quantification_dict_{brain_id}.pickle\"\n",
    "    with open(pkl_file, \"rb\") as handle:\n",
    "        q_dict = pickle.load(handle)\n",
    "    id_to_regioncounts[brain_id] = q_dict\n",
    "\n",
    "    total_somas = 0\n",
    "    for region in q_dict.keys():\n",
    "        total_somas += q_dict[region]\n",
    "    atlas_points[brain_id] = np.zeros((total_somas,0))\n",
    "\n",
    "sd.atlas_points = atlas_points\n",
    "sd.id_to_regioncounts = id_to_regioncounts\n",
    "sd.region_graph = sd._setup_regiongraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.region_barchart(regions, composite_regions=composite_regions, normalize_region=872)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare distributions with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "brain_ids = []\n",
    "genotypes = []\n",
    "\n",
    "for i, brain in enumerate(brains):\n",
    "    print(brain)\n",
    "    region_order = list(df.loc[df[\"Brain ID\"] == brain][\"Region\"])\n",
    "\n",
    "    if i == 0:\n",
    "        standard_region_order = region_order\n",
    "    elif standard_region_order != region_order:\n",
    "        raise ValueError(f\"Different region order for brain {brain}\")\n",
    "\n",
    "    distrib = list(df.loc[df[\"Brain ID\"] == brain][\"Percent of Total Somas (%)\"])\n",
    "    X.append(distrib)\n",
    "\n",
    "    brain_ids.append(brain)\n",
    "    genotypes.append(brains[brain])\n",
    "\n",
    "X = np.array(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2 = pca.fit_transform(X)\n",
    "\n",
    "df_pca = {\n",
    "    \"PC 1\": X_2[:, 0],\n",
    "    \"PC 2\": X_2[:, 1],\n",
    "    \"Genotype\": genotypes,\n",
    "    \"Brain ID\": brain_ids,\n",
    "}\n",
    "df_pca = pd.DataFrame(data=df_pca)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "sns.scatterplot(data=df_pca, x=\"PC 1\", y=\"PC 2\", hue=\"Genotype\", ax=ax)\n",
    "\n",
    "\n",
    "for i in range(df_pca.shape[0]):\n",
    "    plt.text(\n",
    "        x=df_pca[\"PC 1\"][i] + 0.03,\n",
    "        y=df_pca[\"PC 2\"][i] + 0.03,\n",
    "        s=df_pca[\"Brain ID\"][i],\n",
    "        fontdict=dict(color=\"black\", size=20),\n",
    "    )\n",
    "\n",
    "plt.title(\n",
    "    f\"Input Distribution PCA with Explained Variance: {pca.explained_variance_ratio_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create local volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = CloudVolume(\n",
    "    \"precomputed://s3://smartspim-precomputed-volumes/2022_03_10/8531/Ch_647_iso\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subvol = vol[256 * 11 : 256 * 12, 256 * 16 : 256 * 17, 256 * 6 : 256 * 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = CloudVolume.create_new_info(\n",
    "    num_channels=1,\n",
    "    layer_type=\"image\",\n",
    "    data_type=\"uint16\",  # Channel images might be 'uint8'\n",
    "    encoding=\"raw\",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n",
    "    resolution=vol.resolution,  # Voxel scaling, units are in nanometers\n",
    "    voxel_offset=vol.voxel_offset,\n",
    "    chunk_size=vol.chunk_size,  # units are voxels\n",
    "    volume_size=vol.volume_size,  # e.g. a cubic millimeter dataset\n",
    ")\n",
    "\n",
    "vol_ex = CloudVolume(\n",
    "    \"precomputed://file:///Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/brainlit/BrainLine/data/example\",\n",
    "    info=info,\n",
    "    compress=False,\n",
    ")\n",
    "vol_ex.commit_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_ex[256 * 11 : 256 * 12, 256 * 16 : 256 * 17, 256 * 6 : 256 * 7] = subvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "256 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = CloudVolume(\n",
    "    \"precomputed://file:///Users/thomasathey/Documents/mimlab/mouselight/brainlit_parent/brainlit/brainlit/BrainLine/data/example/endogenous\",\n",
    "    fill_missing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('docs_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
