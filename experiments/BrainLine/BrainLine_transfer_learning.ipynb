{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudvolume import CloudVolume\n",
    "from skimage.transform import downscale_local_mean\n",
    "import napari\n",
    "from skimage import io\n",
    "import random\n",
    "import h5py\n",
    "from skimage import measure\n",
    "from brainlit.preprocessing import removeSmallCCs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import tables\n",
    "from napari_animation import AnimationWidget\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from parse_ara import *\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from statannotations.Annotator import Annotator\n",
    "import pandas as pd\n",
    "import brainrender\n",
    "import scipy.ndimage as ndi\n",
    "from skimage.morphology import skeletonize\n",
    "from axon_data import brain2paths as brain2paths_axon\n",
    "from util import find_sample_names\n",
    "from soma_data import brain2paths as brain2paths_soma\n",
    "import os\n",
    "from util import json_to_points\n",
    "import scipy.ndimage as ndi\n",
    "import random\n",
    "from cloudvolume import CloudVolume\n",
    "\n",
    "%gui qt5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Axon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"8649\": ([2192, 1244, 1332], [901, 2559, 1332]),\n",
    "    \"8650\": ([2378, 948, 1851], [2302, 5705, 1851]),\n",
    "    \"8589\": ([4312, 4163, 1450], [1335, 6119, 1450]),\n",
    "    \"8613\": ([3911, 3461, 1514], [1411, 5720, 1514]),\n",
    "    \"8590\": ([1394, 1940, 1854], [4776, 816, 1854]),\n",
    "}  # axon center then nonaxon center\n",
    "\n",
    "data2 = {}\n",
    "for i in [\"8650\", \"8589\", \"8590\"]:\n",
    "    data2[i] = data[i]\n",
    "data = data2\n",
    "\n",
    "intensities = []\n",
    "brain_ids = []\n",
    "vol_type = []\n",
    "# viewer = napari.Viewer(ndisplay=2)\n",
    "\n",
    "\n",
    "for i, brain in enumerate(data.keys()):\n",
    "    vol = CloudVolume(brain2paths_axon[brain][\"base\"] + \"Ch_647\")\n",
    "    coord = data[brain][0]\n",
    "    subvol = vol[\n",
    "        coord[0] - 9 : coord[0] + 10,\n",
    "        coord[1] - 9 : coord[1] + 10,\n",
    "        coord[2] - 9 : coord[2] + 10,\n",
    "    ]\n",
    "    # viewer.add_image(np.squeeze(subvol), name=f\"{brain} axon\", scale = [1.83, 1.83, 2])\n",
    "    num_entries = len(subvol.flatten())\n",
    "    intensities += list(subvol.flatten())\n",
    "    brain_ids += [i] * num_entries\n",
    "    vol_type += [f\"Axon\"] * num_entries\n",
    "\n",
    "    coord = data[brain][1]\n",
    "    subvol = vol[\n",
    "        coord[0] - 9 : coord[0] + 10,\n",
    "        coord[1] - 9 : coord[1] + 10,\n",
    "        coord[2] - 9 : coord[2] + 10,\n",
    "    ]\n",
    "    # viewer.add_image(np.squeeze(subvol), name=f\"{brain} bg\", scale = [1.83, 1.83, 2])\n",
    "    num_entries = len(subvol.flatten())\n",
    "    intensities += list(subvol.flatten())\n",
    "    brain_ids += [i] * num_entries\n",
    "    vol_type += [f\"Background\"] * num_entries\n",
    "\n",
    "# # viewer.camera.angles = [45, 45, 45]\n",
    "# viewer.scale_bar.visible = True\n",
    "# viewer.scale_bar.unit = \"um\"\n",
    "df = pd.DataFrame(\n",
    "    data={\"Intensity\": intensities, \"Subvolume Type\": vol_type, \"Sample\": brain_ids}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.15)\n",
    "fig, axs = plt.subplots(3, 1, sharex=True, dpi=300, figsize=(5, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    hist = sns.histplot(\n",
    "        data=df[df[\"Sample\"] == i],\n",
    "        x=\"Intensity\",\n",
    "        hue=\"Subvolume Type\",\n",
    "        ax=axs[i],\n",
    "        palette=[\"green\", \"red\"],\n",
    "    )\n",
    "    axs[i].set_title(f\"Sample {i}\")\n",
    "    if i > 0:\n",
    "        hist.get_legend().remove()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to sample number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set results\n",
    "training_brain_order = [3, 4, 8649, 8788]\n",
    "models_diff = []\n",
    "models_same = []\n",
    "\n",
    "for i, brain in enumerate(training_brain_order):\n",
    "    model_diff = \"-compare\"\n",
    "    for j, brain2 in enumerate(training_brain_order[: i + 1]):\n",
    "        model_diff += \"-\"\n",
    "        model_diff += str(brain2)\n",
    "\n",
    "        model_same = \"-compare-3\"\n",
    "        if j > 0:\n",
    "            model_same += f\"_{j+1}\"\n",
    "\n",
    "    models_diff.append(model_diff)\n",
    "    models_same.append(model_same)\n",
    "\n",
    "print(f\"heterogeneous: {models_diff} homogeneous: {models_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_order = [8650, 8649, 8613, 8589, 8590, 8788, 8786, 8790, 11537]\n",
    "training_set_sizes = [10, 20, 25, 30]\n",
    "train_sizes = []\n",
    "model_lines = []\n",
    "fscores = []\n",
    "brains = []\n",
    "\n",
    "for brain in tqdm(brain_order, desc=\"Evaluating brains...\"):\n",
    "    brain_dir = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_axon/brain{brain}/\"\n",
    "    # identify images an split into val and test\n",
    "    files = find_sample_names(brain_dir, dset=\"val\", add_dir=False)\n",
    "    random.shuffle(files)\n",
    "    half = int(len(files) / 2)\n",
    "    val_files = files[:half]\n",
    "    test_files = files[half:]\n",
    "\n",
    "    for model_line, models in zip(\n",
    "        [\"Heterogeneous\", \"Homogeneous\"], [models_diff, models_same]\n",
    "    ):\n",
    "        for model, train_size in zip(models, training_set_sizes):\n",
    "            results_dir = brain_dir + \"results\" + model\n",
    "\n",
    "            # val - choose best threshold\n",
    "            true_files = []\n",
    "            pred_files = []\n",
    "            for val_file in val_files:\n",
    "                true_file = (\n",
    "                    f\"{brain_dir}/{val_file.split('.')[0]}-image_2channel_Labels.h5\"\n",
    "                )\n",
    "                true_files.append(true_file)\n",
    "                pred_file = f\"{results_dir}/{val_file.split('.')[0]}_Probabilities.h5\"\n",
    "                pred_files.append(pred_file)\n",
    "\n",
    "            spacing = 0.02\n",
    "            thresholds = np.arange(spacing, 1.0, spacing)\n",
    "            best_fscore = 0\n",
    "            best_thresh = -1\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                true_pos_total = 0\n",
    "                false_pos_total = 0\n",
    "                true_labels_total = 0\n",
    "\n",
    "                for true_file, pred_file in zip(true_files, pred_files):\n",
    "                    f = h5py.File(pred_file, \"r\")\n",
    "                    seg = f.get(\"exported_data\")\n",
    "                    seg = seg[1, :, :, :]\n",
    "                    mask_forward = seg > threshold\n",
    "\n",
    "                    f = h5py.File(true_file, \"r\")\n",
    "                    gt = f.get(\"exported_data\")\n",
    "                    gt = gt[0, :, :, :]\n",
    "                    pos_labels = gt == 2\n",
    "                    neg_labels = gt == 1\n",
    "\n",
    "                    true_pos = np.sum(np.logical_and(mask_forward, pos_labels))\n",
    "                    true_pos_total += true_pos\n",
    "\n",
    "                    false_pos = np.sum(np.logical_and(mask_forward, gt == 1))\n",
    "                    false_pos_total += false_pos\n",
    "\n",
    "                    true_labels = np.sum(pos_labels)\n",
    "                    true_labels_total += true_labels\n",
    "\n",
    "                precision_total = true_pos_total / (true_pos_total + false_pos_total)\n",
    "                recall_total = true_pos_total / true_labels_total\n",
    "                fscore = 2 / (1 / precision_total + 1 / recall_total)\n",
    "\n",
    "                if fscore > best_fscore:\n",
    "                    best_fscore = fscore\n",
    "                    best_thresh = threshold\n",
    "\n",
    "            # test\n",
    "            true_files = []\n",
    "            pred_files = []\n",
    "            for val_file in test_files:\n",
    "                true_file = (\n",
    "                    f\"{brain_dir}/{val_file.split('.')[0]}-image_2channel_Labels.h5\"\n",
    "                )\n",
    "                true_files.append(true_file)\n",
    "                pred_file = f\"{results_dir}/{val_file.split('.')[0]}_Probabilities.h5\"\n",
    "                pred_files.append(pred_file)\n",
    "\n",
    "            true_pos_total = 0\n",
    "            false_pos_total = 0\n",
    "            true_labels_total = 0\n",
    "\n",
    "            for true_file, pred_file in zip(true_files, pred_files):\n",
    "                f = h5py.File(pred_file, \"r\")\n",
    "                seg = f.get(\"exported_data\")\n",
    "                seg = seg[1, :, :, :]\n",
    "                mask_forward = seg > best_thresh\n",
    "\n",
    "                f = h5py.File(true_file, \"r\")\n",
    "                gt = f.get(\"exported_data\")\n",
    "                gt = gt[0, :, :, :]\n",
    "                pos_labels = gt == 2\n",
    "                neg_labels = gt == 1\n",
    "\n",
    "                true_pos = np.sum(np.logical_and(mask_forward, pos_labels))\n",
    "                true_pos_total += true_pos\n",
    "\n",
    "                false_pos = np.sum(np.logical_and(mask_forward, gt == 1))\n",
    "                false_pos_total += false_pos\n",
    "\n",
    "                true_labels = np.sum(pos_labels)\n",
    "                true_labels_total += true_labels\n",
    "\n",
    "            precision_total = true_pos_total / (true_pos_total + false_pos_total)\n",
    "            recall_total = true_pos_total / true_labels_total\n",
    "            fscore = 2 / (1 / precision_total + 1 / recall_total)\n",
    "\n",
    "            fscores.append(fscore)\n",
    "            train_sizes.append(train_size)\n",
    "            brains.append(brain)\n",
    "\n",
    "            model_lines.append(model_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Test F-Score\": fscores,\n",
    "        \"Number of Subvolumes in Training Set\": train_sizes,\n",
    "        \"Brain\": brains,\n",
    "        \"Training Type\": model_lines,\n",
    "    }\n",
    ")\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5), dpi=300)\n",
    "sns.lineplot(\n",
    "    data=df,\n",
    "    x=\"Number of Subvolumes in Training Set\",\n",
    "    y=\"Test F-Score\",\n",
    "    hue=\"Brain\",\n",
    "    style=\"Training Type\",\n",
    "    ax=axes,\n",
    "    alpha=0.5,\n",
    "    linestyle=\"dashed\",\n",
    "    palette=\"tab10\",\n",
    ")\n",
    "\n",
    "\n",
    "# axes.plot(np.arange(len(models)), av_fscores, color='red', label=\"Average\")\n",
    "# leg = axes.legend(loc='lower right')\n",
    "# leg_lines = leg.get_lines()\n",
    "# for i in range(len(brain_order)):\n",
    "#     leg_lines[i].set_linestyle(\"--\")\n",
    "axes.set_xticks(training_set_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Test F-Score\": fscores,\n",
    "        \"Number of Subvolumes in Training Set\": train_sizes,\n",
    "        \"Brain\": brains,\n",
    "        \"Training Type\": model_lines,\n",
    "    }\n",
    ")\n",
    "sns.set(font_scale=1.15)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5), dpi=300)\n",
    "sns.lineplot(\n",
    "    data=df,\n",
    "    x=\"Number of Subvolumes in Training Set\",\n",
    "    y=\"Test F-Score\",\n",
    "    hue=\"Training Type\",\n",
    ")\n",
    "axes.set_xticks(training_set_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"8606\": ([1726, 2619, 1687], [3071, 4360, 1687]),\n",
    "    \"8446\": ([1979, 3491, 1791], [1258, 3502, 1791]),\n",
    "    \"887\": ([3537, 3787, 1385], [2088, 1924, 1385]),\n",
    "}  # soma center then nonsoma center\n",
    "\n",
    "intensities = []\n",
    "brain_ids = []\n",
    "vol_type = []\n",
    "\n",
    "for i, brain in enumerate(data.keys()):\n",
    "    vol = CloudVolume(brain2paths_soma[brain][\"base\"] + \"Ch_647\")\n",
    "    coord = data[brain][0]\n",
    "    subvol = np.array(\n",
    "        vol[\n",
    "            coord[0] - 9 : coord[0] + 10,\n",
    "            coord[1] - 9 : coord[1] + 10,\n",
    "            coord[2] - 9 : coord[2] + 10,\n",
    "        ],\n",
    "        dtype=\"float\",\n",
    "    )\n",
    "    num_entries = len(subvol.flatten())\n",
    "    intensities += list(subvol.flatten())\n",
    "    brain_ids += [i] * num_entries\n",
    "    vol_type += [\"Soma\"] * num_entries\n",
    "\n",
    "    coord = data[brain][1]\n",
    "    subvol = np.array(\n",
    "        vol[\n",
    "            coord[0] - 9 : coord[0] + 10,\n",
    "            coord[1] - 9 : coord[1] + 10,\n",
    "            coord[2] - 9 : coord[2] + 10,\n",
    "        ],\n",
    "        dtype=\"float\",\n",
    "    )\n",
    "    num_entries = len(subvol.flatten())\n",
    "    intensities += list(subvol.flatten())\n",
    "    brain_ids += [i] * num_entries\n",
    "    vol_type += [\"Background\"] * num_entries\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data={\"Intensity\": intensities, \"Subvolume Type\": vol_type, \"Sample\": brain_ids}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.15)\n",
    "fig, axs = plt.subplots(3, 1, sharex=True, dpi=300, figsize=(5, 5))\n",
    "\n",
    "for i in range(3):\n",
    "    hist = sns.histplot(\n",
    "        data=df[df[\"Sample\"] == i],\n",
    "        x=\"Intensity\",\n",
    "        hue=\"Subvolume Type\",\n",
    "        ax=axs[i],\n",
    "        palette=[\"green\", \"red\"],\n",
    "    )\n",
    "    axs[i].set_title(f\"Sample {i}\")\n",
    "    if i > 0:\n",
    "        hist.get_legend().remove()\n",
    "\n",
    "axs[0].set_xlim(-500, 8000)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to sample number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set results\n",
    "training_brain_order = [\"r1\", \"r2\", \"878\", \"887\"]\n",
    "models_diff = []\n",
    "models_same = []\n",
    "\n",
    "for i, brain in enumerate(training_brain_order):\n",
    "    model_diff = \"-compare\"\n",
    "    for j, brain2 in enumerate(training_brain_order[: i + 1]):\n",
    "        model_diff += \"-\"\n",
    "        model_diff += str(brain2)\n",
    "\n",
    "        model_same = \"-compare-r1\"\n",
    "        if j > 0:\n",
    "            model_same += f\"_{j+1}\"\n",
    "\n",
    "    models_diff.append(model_diff)\n",
    "    models_same.append(model_same)\n",
    "\n",
    "print(f\"heterogeneous: {models_diff} homogeneous: {models_same}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubles = [\n",
    "    \"3972_1636_1575_pos_Probabilities.h5\",\n",
    "    \"2867_4336_1296_pos_Probabilities.h5\",\n",
    "    \"2607_1845_1309_pos_Probabilities.h5\",\n",
    "    \"2101_3397_1747_pos_Probabilities.h5\",\n",
    "    \"2011_3452_1911_pos_Probabilities.h5\",\n",
    "    \"2113_3353_1727_pos_Probabilities.h5\",\n",
    "    \"1968_3472_1784_pos_Probabilities.h5\",\n",
    "]  # 8446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_order = [\n",
    "    \"8607\",\n",
    "    \"8606\",\n",
    "    \"8477\",\n",
    "    \"8531\",\n",
    "    \"8608\",\n",
    "    \"8529\",\n",
    "    \"8557\",\n",
    "    \"8555\",\n",
    "    \"8446\",\n",
    "    \"8454\",\n",
    "    \"887\",\n",
    "]\n",
    "training_set_sizes = [25, 45, 51, 59]\n",
    "train_sizes = []\n",
    "model_lines = []\n",
    "fscores = []\n",
    "brains = []\n",
    "size_thresh = 500\n",
    "\n",
    "for brain in tqdm(brain_order, desc=\"Evaluating brains...\"):\n",
    "    if brain == \"8557\":\n",
    "        brain_name = \"r1\"\n",
    "    elif brain == \"8555\":\n",
    "        brain_name = \"r2\"\n",
    "    else:\n",
    "        brain_name = brain\n",
    "\n",
    "    brain_dir = f\"/Users/thomasathey/Documents/mimlab/mouselight/ailey/detection_soma/brain{brain_name}/val/\"\n",
    "    # identify images an split into val and test\n",
    "    files = find_sample_names(brain_dir, dset=\"\", add_dir=False)\n",
    "    random.shuffle(files)\n",
    "    half = int(len(files) / 2)\n",
    "    val_files = files[:half]\n",
    "    test_files = files[half:]\n",
    "\n",
    "    for model_line, models in zip(\n",
    "        [\"Heterogeneous\", \"Homogeneous\"], [models_diff, models_same]\n",
    "    ):\n",
    "        for model, train_size in zip(models, training_set_sizes):\n",
    "            results_dir = brain_dir + \"results\" + model + \"/\"\n",
    "\n",
    "            # val - choose best threshold\n",
    "            pred_files = []\n",
    "            for val_file in val_files:\n",
    "                pred_file = f\"{results_dir}{val_file.split('.')[0]}_Probabilities.h5\"\n",
    "                pred_files.append(pred_file)\n",
    "\n",
    "            spacing = 0.02\n",
    "            thresholds = np.arange(spacing, 1.0, spacing)\n",
    "            best_fscore = 0\n",
    "            best_thresh = -1\n",
    "\n",
    "            for threshold in thresholds:\n",
    "                tot_pos = 0\n",
    "                true_pos = 0\n",
    "                false_pos = 0\n",
    "\n",
    "                for pred_file in pred_files:\n",
    "                    if pred_file in doubles:\n",
    "                        newpos = 2\n",
    "                    else:\n",
    "                        newpos = 1\n",
    "\n",
    "                    f = h5py.File(pred_file, \"r\")\n",
    "                    seg = f.get(\"exported_data\")\n",
    "                    seg = seg[0, :, :, :]\n",
    "                    mask = seg > threshold\n",
    "                    labels = measure.label(mask)\n",
    "                    props = measure.regionprops(labels)\n",
    "\n",
    "                    if \"pos\" in pred_file:\n",
    "                        num_detected = 0\n",
    "                        tot_pos += newpos\n",
    "                        for prop in props:\n",
    "                            if prop[\"area\"] > size_thresh:\n",
    "                                if num_detected < newpos:\n",
    "                                    true_pos += 1\n",
    "                                    num_detected += 1\n",
    "                                else:\n",
    "                                    false_pos += 1\n",
    "                    elif \"neg\" in pred_file:\n",
    "                        for prop in props:\n",
    "                            if prop[\"area\"] > size_thresh:\n",
    "                                false_pos += 1\n",
    "                if tot_pos == 0:\n",
    "                    recall = 1\n",
    "                else:\n",
    "                    recall = true_pos / tot_pos\n",
    "\n",
    "                if true_pos + false_pos == 0:\n",
    "                    precision = 0\n",
    "                else:\n",
    "                    precision = true_pos / (true_pos + false_pos)\n",
    "\n",
    "                if precision == 0 and recall == 0:\n",
    "                    fscore = 0\n",
    "                else:\n",
    "                    fscore = 2 / (1 / precision + 1 / recall)\n",
    "\n",
    "                if fscore > best_fscore:\n",
    "                    best_fscore = fscore\n",
    "                    best_thresh = threshold\n",
    "\n",
    "            # test\n",
    "            pred_files = []\n",
    "            for val_file in test_files:\n",
    "                pred_file = f\"{results_dir}/{val_file.split('.')[0]}_Probabilities.h5\"\n",
    "                pred_files.append(pred_file)\n",
    "\n",
    "            tot_pos = 0\n",
    "            true_pos = 0\n",
    "            false_pos = 0\n",
    "\n",
    "            for pred_file in pred_files:\n",
    "                if pred_file in doubles:\n",
    "                    newpos = 2\n",
    "                else:\n",
    "                    newpos = 1\n",
    "\n",
    "                f = h5py.File(pred_file, \"r\")\n",
    "                seg = f.get(\"exported_data\")\n",
    "                seg = seg[0, :, :, :]\n",
    "                mask = seg > best_thresh\n",
    "                labels = measure.label(mask)\n",
    "                props = measure.regionprops(labels)\n",
    "\n",
    "                if \"pos\" in pred_file:\n",
    "                    num_detected = 0\n",
    "                    tot_pos += newpos\n",
    "                    for prop in props:\n",
    "                        if prop[\"area\"] > size_thresh:\n",
    "                            if num_detected < newpos:\n",
    "                                true_pos += 1\n",
    "                                num_detected += 1\n",
    "                            else:\n",
    "                                false_pos += 1\n",
    "                elif \"neg\" in pred_file:\n",
    "                    for prop in props:\n",
    "                        if prop[\"area\"] > size_thresh:\n",
    "                            false_pos += 1\n",
    "\n",
    "            if tot_pos == 0:\n",
    "                recall = 1\n",
    "            else:\n",
    "                recall = true_pos / tot_pos\n",
    "\n",
    "            if true_pos + false_pos == 0:\n",
    "                precision = 0\n",
    "            else:\n",
    "                precision = true_pos / (true_pos + false_pos)\n",
    "\n",
    "            if precision == 0 and recall == 0:\n",
    "                fscore = 0\n",
    "            else:\n",
    "                fscore = 2 / (1 / precision + 1 / recall)\n",
    "\n",
    "            fscores.append(fscore)\n",
    "            train_sizes.append(train_size)\n",
    "            brains.append(brain)\n",
    "\n",
    "            model_lines.append(model_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Test F-Score\": fscores,\n",
    "        \"Number of Subvolumes in Training Set\": train_sizes,\n",
    "        \"Brain\": brains,\n",
    "        \"Training Type\": model_lines,\n",
    "    }\n",
    ")\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5), dpi=300)\n",
    "sns.lineplot(\n",
    "    data=df,\n",
    "    x=\"Number of Subvolumes in Training Set\",\n",
    "    y=\"Test F-Score\",\n",
    "    hue=\"Brain\",\n",
    "    style=\"Training Type\",\n",
    "    ax=axes,\n",
    "    alpha=0.5,\n",
    "    linestyle=\"dashed\",\n",
    "    palette=\"tab10\",\n",
    ")\n",
    "\n",
    "\n",
    "# axes.plot(np.arange(len(models)), av_fscores, color='red', label=\"Average\")\n",
    "# leg = axes.legend(loc='lower right')\n",
    "# leg_lines = leg.get_lines()\n",
    "# for i in range(len(brain_order)):\n",
    "#     leg_lines[i].set_linestyle(\"--\")\n",
    "axes.set_xticks(training_set_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"Test F-Score\": fscores,\n",
    "        \"Number of Subvolumes in Training Set\": train_sizes,\n",
    "        \"Brain\": brains,\n",
    "        \"Training Type\": model_lines,\n",
    "    }\n",
    ")\n",
    "sns.set(font_scale=1.15)\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 5), dpi=300)\n",
    "sns.lineplot(\n",
    "    data=df,\n",
    "    x=\"Number of Subvolumes in Training Set\",\n",
    "    y=\"Test F-Score\",\n",
    "    hue=\"Training Type\",\n",
    ")\n",
    "axes.set_xticks(training_set_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('docs_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dc00d68ff54f8375e99934614da4863299fb9e10af4294c095b7f517546ff26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
